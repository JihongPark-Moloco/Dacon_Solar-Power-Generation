{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"XGB+LGBM+MLP_Staking.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"13Yq14RRcw9eeQDpQh-MS4sd3qpl2reoZ","authorship_tag":"ABX9TyOzttaHu2I6rarN+nDkuG00"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"xUXyLa2O5Ctc"},"source":["import pandas as pd\r\n","import numpy as np\r\n","import os\r\n","import glob\r\n","import random\r\n","\r\n","import warnings\r\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"73pJ3lKo5DPq"},"source":["train = pd.read_csv('/content/drive/MyDrive/Dacon/train/train.csv')\r\n","submission = pd.read_csv('/content/drive/MyDrive/Dacon/sample_submission.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pum7_CrA5DQZ"},"source":["# data = train.csv\r\n","# lags = [48]\r\n","# cols = ['TARGET'], ['DHI', 'DNI', 'WS', 'RH', 'T']\r\n","\r\n","def create_lag_feats(data, lags, cols):\r\n","    \r\n","    lag_cols = []\r\n","    temp = data.copy()\r\n","    for col in cols:\r\n","        for lag in lags:\r\n","            # 48칸 이동\r\n","            temp[col + '_lag_%s'%lag] = temp[col].shift(lag)\r\n","            # Target1 = 원래 TARGET값\r\n","            # Target2 = 1일 앞으로 댕겼음\r\n","            temp['Target1'] = temp['TARGET']\r\n","            temp['Target2'] = temp['TARGET'].shift(-48).fillna(method='ffill')\r\n","            # lag_cols = 1일 뒤로 밀었음\r\n","            lag_cols.append(col + '_lag_%s'%lag)\r\n","\r\n","    return temp, lag_cols    \r\n","\r\n","def preprocess_data(data, target_lags=[48], weather_lags=[48], is_train=True):\r\n","    \r\n","    temp = data.copy()\r\n","\r\n","    if is_train==True:          \r\n","    \r\n","        temp, temp_lag_cols1 = create_lag_feats(temp, target_lags, ['TARGET'])\r\n","        temp, temp_lag_cols2 = create_lag_feats(temp, weather_lags, ['DHI', 'DNI', 'WS', 'RH', 'T'])\r\n","     \r\n","        return temp[['Hour'] + temp_lag_cols1 + temp_lag_cols2 + ['Target1', 'Target2']].dropna()\r\n","\r\n","    # Target1, Target2 제거\r\n","    elif is_train==False:    \r\n","        \r\n","        temp, temp_lag_cols1 = create_lag_feats(temp, target_lags, ['TARGET'])\r\n","        temp, temp_lag_cols2 = create_lag_feats(temp, weather_lags, ['DHI', 'DNI', 'WS', 'RH', 'T'])\r\n","                              \r\n","        return temp[['Hour'] + temp_lag_cols1 + temp_lag_cols2].dropna()\r\n","\r\n","\r\n","df_train = preprocess_data(train, target_lags=[48], weather_lags=[48], is_train=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BGyB6PAy5DRD","executionInfo":{"status":"ok","timestamp":1610182003657,"user_tz":-540,"elapsed":2434,"user":{"displayName":"정성헌","photoUrl":"","userId":"12794350723891602651"}},"outputId":"e96475b5-64fd-4de6-a3aa-cf88e09611ed"},"source":["df_test = []\r\n","\r\n","for i in range(81):\r\n","    file_path = '/content/drive/MyDrive/Dacon//test/' + str(i) + '.csv'\r\n","    temp = pd.read_csv(file_path)\r\n","    # 뒤에서부터 48개만 들고온다.\r\n","    temp = preprocess_data(temp, target_lags=[48], weather_lags=[48], is_train=False).iloc[-48:]\r\n","    df_test.append(temp)\r\n","\r\n","X_test = pd.concat(df_test)\r\n","X_test.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3888, 7)"]},"metadata":{"tags":[]},"execution_count":121}]},{"cell_type":"code","metadata":{"id":"lnEKoAHK5DR8"},"source":["from sklearn.model_selection import train_test_split\r\n","\r\n","# Y_train_1 = Target1\r\n","# Y_train_2 = Target2(Target2 = 1일 앞으로 댕겼음)\r\n","X_train_1, X_valid_1, Y_train_1, Y_valid_1 = train_test_split(df_train.iloc[:, :-2], df_train.iloc[:, -2], test_size=0.3, random_state=0)\r\n","X_train_2, X_valid_2, Y_train_2, Y_valid_2 = train_test_split(df_train.iloc[:, :-2], df_train.iloc[:, -1], test_size=0.3, random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W0qhl8AR5DSm"},"source":["quantiles = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1_xRb6F85UMa"},"source":["# LGBM"]},{"cell_type":"code","metadata":{"id":"SmgRZYeB5Dbz"},"source":["from lightgbm import LGBMRegressor\r\n","\r\n","# Get the model and the predictions in (a) - (b)\r\n","def LGBM(q, X_train, Y_train, X_valid, Y_valid, X_test):    \r\n","    # (a) Modeling  \r\n","    model = LGBMRegressor(objective='quantile', alpha=q, seed=2021,\r\n","                         n_estimators=10000, bagging_fraction=0.7, learning_rate=0.027, subsample=0.7)                   \r\n","                         \r\n","    model.fit(X_train, Y_train, eval_metric = ['quantile'], \r\n","          eval_set=[(X_valid, Y_valid)], early_stopping_rounds=300, verbose=500)\r\n","\r\n","    # (b) Predictions\r\n","    pred = pd.Series(model.predict(X_test).round(2))\r\n","    return model, pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TvdO5LRp5Dct"},"source":["# Target 예측\r\n","\r\n","def train_data_LGBM(X_train, Y_train, X_valid, Y_valid, X_test):\r\n","\r\n","    LGBM_models=[]\r\n","    LGBM_actual_pred = pd.DataFrame()\r\n","\r\n","    for q in quantiles:\r\n","        print(q)\r\n","        model, pred = LGBM(q, X_train, Y_train, X_valid, Y_valid, X_test)\r\n","        LGBM_models.append(model)\r\n","        LGBM_actual_pred = pd.concat([LGBM_actual_pred,pred],axis=1)\r\n","\r\n","    LGBM_actual_pred.columns=quantiles\r\n","    \r\n","    return LGBM_models, LGBM_actual_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D8SaN_9d5Ddr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610172507859,"user_tz":-540,"elapsed":116893,"user":{"displayName":"정성헌","photoUrl":"","userId":"12794350723891602651"}},"outputId":"623a7d3a-0740-48e7-9c9a-f1200b27233a"},"source":["# Target1\r\n","LGBM_1, LGBM_results_1 = train_data_LGBM(X_train_1, Y_train_1, X_valid_1, Y_valid_1, X_test)\r\n","LGBM_2, LGBM_results_2 = train_data_LGBM(X_train_2, Y_train_2, X_valid_2, Y_valid_2, X_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.1\n","Training until validation scores don't improve for 300 rounds.\n","[500]\tvalid_0's quantile: 1.37212\n","Early stopping, best iteration is:\n","[676]\tvalid_0's quantile: 1.37108\n","0.2\n","Training until validation scores don't improve for 300 rounds.\n","[500]\tvalid_0's quantile: 2.16051\n","[1000]\tvalid_0's quantile: 2.14876\n","[1500]\tvalid_0's quantile: 2.14286\n","[2000]\tvalid_0's quantile: 2.13669\n","[2500]\tvalid_0's quantile: 2.13719\n","Early stopping, best iteration is:\n","[2316]\tvalid_0's quantile: 2.13589\n","0.3\n","Training until validation scores don't improve for 300 rounds.\n","[500]\tvalid_0's quantile: 2.57354\n","[1000]\tvalid_0's quantile: 2.54514\n","[1500]\tvalid_0's quantile: 2.53945\n","[2000]\tvalid_0's quantile: 2.53633\n","[2500]\tvalid_0's quantile: 2.53497\n","[3000]\tvalid_0's quantile: 2.53294\n","[3500]\tvalid_0's quantile: 2.5275\n","Early stopping, best iteration is:\n","[3505]\tvalid_0's quantile: 2.5274\n","0.4\n","Training until validation scores don't improve for 300 rounds.\n","[500]\tvalid_0's quantile: 2.6792\n","[1000]\tvalid_0's quantile: 2.65925\n","[1500]\tvalid_0's quantile: 2.64882\n","[2000]\tvalid_0's quantile: 2.64038\n","[2500]\tvalid_0's quantile: 2.63707\n","[3000]\tvalid_0's quantile: 2.62902\n","[3500]\tvalid_0's quantile: 2.6248\n","[4000]\tvalid_0's quantile: 2.62081\n","[4500]\tvalid_0's quantile: 2.61785\n","[5000]\tvalid_0's quantile: 2.61548\n","[5500]\tvalid_0's quantile: 2.61465\n","Early stopping, best iteration is:\n","[5375]\tvalid_0's quantile: 2.61414\n","0.5\n","Training until validation scores don't improve for 300 rounds.\n","[500]\tvalid_0's quantile: 2.58457\n","[1000]\tvalid_0's quantile: 2.56152\n","[1500]\tvalid_0's quantile: 2.55218\n","[2000]\tvalid_0's quantile: 2.53971\n","[2500]\tvalid_0's quantile: 2.53316\n","[3000]\tvalid_0's quantile: 2.52699\n","[3500]\tvalid_0's quantile: 2.52443\n","[4000]\tvalid_0's quantile: 2.52193\n","[4500]\tvalid_0's quantile: 2.5191\n","[5000]\tvalid_0's quantile: 2.51742\n","[5500]\tvalid_0's quantile: 2.51699\n","[6000]\tvalid_0's quantile: 2.51643\n","[6500]\tvalid_0's quantile: 2.51601\n","Early stopping, best iteration is:\n","[6393]\tvalid_0's quantile: 2.5159\n","0.6\n","Training until validation scores don't improve for 300 rounds.\n","[500]\tvalid_0's quantile: 2.29981\n","[1000]\tvalid_0's quantile: 2.28368\n","[1500]\tvalid_0's quantile: 2.27146\n","[2000]\tvalid_0's quantile: 2.26553\n","[2500]\tvalid_0's quantile: 2.2623\n","[3000]\tvalid_0's quantile: 2.25653\n","[3500]\tvalid_0's quantile: 2.25286\n","[4000]\tvalid_0's quantile: 2.25211\n","[4500]\tvalid_0's quantile: 2.24752\n","[5000]\tvalid_0's quantile: 2.24517\n","[5500]\tvalid_0's quantile: 2.24546\n","Early stopping, best iteration is:\n","[5209]\tvalid_0's quantile: 2.24498\n","0.7\n","Training until validation scores don't improve for 300 rounds.\n","[500]\tvalid_0's quantile: 1.89842\n","[1000]\tvalid_0's quantile: 1.8814\n","[1500]\tvalid_0's quantile: 1.87243\n","[2000]\tvalid_0's quantile: 1.8693\n","[2500]\tvalid_0's quantile: 1.86746\n","[3000]\tvalid_0's quantile: 1.86702\n","Early stopping, best iteration is:\n","[3146]\tvalid_0's quantile: 1.86645\n","0.8\n","Training until validation scores don't improve for 300 rounds.\n","[500]\tvalid_0's quantile: 1.37877\n","[1000]\tvalid_0's quantile: 1.36884\n","[1500]\tvalid_0's quantile: 1.36499\n","[2000]\tvalid_0's quantile: 1.36239\n","[2500]\tvalid_0's quantile: 1.35952\n","[3000]\tvalid_0's quantile: 1.35845\n","Early stopping, best iteration is:\n","[2909]\tvalid_0's quantile: 1.35827\n","0.9\n","Training until validation scores don't improve for 300 rounds.\n","[500]\tvalid_0's quantile: 0.765182\n","[1000]\tvalid_0's quantile: 0.760302\n","[1500]\tvalid_0's quantile: 0.75859\n","[2000]\tvalid_0's quantile: 0.757697\n","Early stopping, best iteration is:\n","[1949]\tvalid_0's quantile: 0.757586\n","0.1\n","Training until validation scores don't improve for 300 rounds.\n","Early stopping, best iteration is:\n","[182]\tvalid_0's quantile: 1.4093\n","0.2\n","Training until validation scores don't improve for 300 rounds.\n","[500]\tvalid_0's quantile: 2.2503\n","[1000]\tvalid_0's quantile: 2.23588\n","Early stopping, best iteration is:\n","[1173]\tvalid_0's quantile: 2.23198\n","0.3\n","Training until validation scores don't improve for 300 rounds.\n","[500]\tvalid_0's quantile: 2.66486\n","[1000]\tvalid_0's quantile: 2.64927\n","[1500]\tvalid_0's quantile: 2.63853\n","Early stopping, best iteration is:\n","[1398]\tvalid_0's quantile: 2.63812\n","0.4\n","Training until validation scores don't improve for 300 rounds.\n","[500]\tvalid_0's quantile: 2.8087\n","[1000]\tvalid_0's quantile: 2.77833\n","[1500]\tvalid_0's quantile: 2.76475\n","Early stopping, best iteration is:\n","[1651]\tvalid_0's quantile: 2.76252\n","0.5\n","Training until validation scores don't improve for 300 rounds.\n","[500]\tvalid_0's quantile: 2.72555\n","[1000]\tvalid_0's quantile: 2.6931\n","Early stopping, best iteration is:\n","[1174]\tvalid_0's quantile: 2.68763\n","0.6\n","Training until validation scores don't improve for 300 rounds.\n","[500]\tvalid_0's quantile: 2.44344\n","[1000]\tvalid_0's quantile: 2.42378\n","[1500]\tvalid_0's quantile: 2.41561\n","[2000]\tvalid_0's quantile: 2.40552\n","[2500]\tvalid_0's quantile: 2.40075\n","[3000]\tvalid_0's quantile: 2.39581\n","[3500]\tvalid_0's quantile: 2.3924\n","[4000]\tvalid_0's quantile: 2.3911\n","[4500]\tvalid_0's quantile: 2.39006\n","[5000]\tvalid_0's quantile: 2.38906\n","[5500]\tvalid_0's quantile: 2.38838\n","Early stopping, best iteration is:\n","[5397]\tvalid_0's quantile: 2.38808\n","0.7\n","Training until validation scores don't improve for 300 rounds.\n","[500]\tvalid_0's quantile: 1.99723\n","[1000]\tvalid_0's quantile: 1.98056\n","[1500]\tvalid_0's quantile: 1.97825\n","[2000]\tvalid_0's quantile: 1.97001\n","[2500]\tvalid_0's quantile: 1.96528\n","[3000]\tvalid_0's quantile: 1.9625\n","[3500]\tvalid_0's quantile: 1.96135\n","[4000]\tvalid_0's quantile: 1.96092\n","[4500]\tvalid_0's quantile: 1.95981\n","[5000]\tvalid_0's quantile: 1.95917\n","Early stopping, best iteration is:\n","[5060]\tvalid_0's quantile: 1.95905\n","0.8\n","Training until validation scores don't improve for 300 rounds.\n","[500]\tvalid_0's quantile: 1.43443\n","[1000]\tvalid_0's quantile: 1.42706\n","[1500]\tvalid_0's quantile: 1.42463\n","Early stopping, best iteration is:\n","[1363]\tvalid_0's quantile: 1.42409\n","0.9\n","Training until validation scores don't improve for 300 rounds.\n","[500]\tvalid_0's quantile: 0.793532\n","[1000]\tvalid_0's quantile: 0.789823\n","[1500]\tvalid_0's quantile: 0.789239\n","Early stopping, best iteration is:\n","[1334]\tvalid_0's quantile: 0.789074\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lqGTTfWqyebM"},"source":["# RESULT_1"]},{"cell_type":"code","metadata":{"id":"GVwsAhOm5DfO"},"source":["x_train_1 = df_train.iloc[:, :-2]\r\n","y_train_1 = df_train.iloc[:, -2]\r\n","x_train_2 = df_train.iloc[:, :-2]\r\n","y_train_2 = df_train.iloc[:, -1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G-ktLmPT5DgF"},"source":["LGBM_predict_1 = pd.DataFrame()\r\n","for i in range(9):\r\n","    print('☆☆☆☆☆☆☆☆☆☆ ', i*0.1+0.1, ' ☆☆☆☆☆☆☆☆☆☆')\r\n","    pred = LGBM_1[i].predict(x_train_1)\r\n","    pred = pd.Series(pred)\r\n","    LGBM_predict_1 = pd.concat([LGBM_predict_1,pred],axis=1)\r\n","LGBM_predict_1.columns = ['0.1','0.2','0.3','0.4','0.5','0.6','0.7','0.8','0.9']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aOv6IfxF5DhC"},"source":["LGBM_predict_2 = pd.DataFrame()\r\n","for i in range(9):\r\n","    print('☆☆☆☆☆☆☆☆☆☆ ', i*0.1+0.1, ' ☆☆☆☆☆☆☆☆☆☆')\r\n","    pred = LGBM_2[i].predict(x_train_2)\r\n","    pred = pd.Series(pred)\r\n","    LGBM_predict_2 = pd.concat([LGBM_predict_2,pred],axis=1)\r\n","LGBM_predict_2.columns = ['0.1','0.2','0.3','0.4','0.5','0.6','0.7','0.8','0.9']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XX9-EjYuydV6"},"source":["LGBM_test_result_1 = pd.DataFrame()\r\n","for i in range(9):\r\n","    print('☆☆☆☆☆☆☆☆☆☆ ', i*0.1+0.1, ' ☆☆☆☆☆☆☆☆☆☆')\r\n","    pred = LGBM_1[i].predict(X_test)\r\n","    pred = pd.Series(pred)\r\n","    LGBM_test_result_1 = pd.concat([LGBM_test_result_1,pred],axis=1)\r\n","LGBM_test_result_1.columns = ['0.1','0.2','0.3','0.4','0.5','0.6','0.7','0.8','0.9']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TU7nBhNbydYs"},"source":["LGBM_test_result_2 = pd.DataFrame()\r\n","for i in range(9):\r\n","    print('☆☆☆☆☆☆☆☆☆☆ ', i*0.1+0.1, ' ☆☆☆☆☆☆☆☆☆☆')\r\n","    pred = LGBM_2[i].predict(X_test)\r\n","    pred = pd.Series(pred)\r\n","    LGBM_test_result_2 = pd.concat([LGBM_test_result_2,pred],axis=1)\r\n","LGBM_test_result_2.columns = ['0.1','0.2','0.3','0.4','0.5','0.6','0.7','0.8','0.9']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZvJtyn035QTN"},"source":["LGBM_predict_1.to_csv('/content/drive/MyDrive/Dacon/LGBM_predict_1.csv', index=False)\r\n","LGBM_predict_2.to_csv('/content/drive/MyDrive/Dacon/LGBM_predict_2.csv', index=False)\r\n","LGBM_test_result_1.to_csv('/content/drive/MyDrive/Dacon/LGBM_test_result_1.csv', index=False)\r\n","LGBM_test_result_2.to_csv('/content/drive/MyDrive/Dacon/LGBM_test_result_2.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8vmzJtGw5efi"},"source":["# XGB"]},{"cell_type":"code","metadata":{"id":"QlnIbrf-5Dh3"},"source":["def xgb_quantile_eval(quantile):\r\n","    def loss(preds, dmatrix):\r\n","        labels = dmatrix.get_label()\r\n","        return ('q{}_loss'.format(quantile),\r\n","                np.nanmean((preds >= labels) * (1 - quantile) * (preds - labels) +\r\n","                           (preds < labels) * quantile * (labels - preds)))\r\n","    return loss\r\n","\r\n","def xgb_quantile_obj(quantile):\r\n","    def loss(labels, preds):\r\n","        try:\r\n","            assert 0 <= quantile <= 1\r\n","        except AssertionError:\r\n","            raise ValueError(\"Quantile value must be float between 0 and 1.\")\r\n","\r\n","        errors = preds - labels\r\n","\r\n","        left_mask = errors < 0\r\n","        right_mask = errors > 0\r\n","\r\n","        grad = -quantile * left_mask + (1 - quantile) * right_mask\r\n","        hess = np.ones_like(preds)\r\n","\r\n","        return grad, hess\r\n","    return loss\r\n","\r\n","# bst = xgb.train(hyperparams, train, num_rounds,\r\n","#                 obj=xgb_quantile_obj, feval=xgb_quantile_eval)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DmQV-jRN5Din"},"source":["from xgboost import XGBRegressor\r\n","import xgboost as xgb\r\n","\r\n","# Get the model and the predictions in (a) - (b)\r\n","def XGB(q, X_train, Y_train, X_valid, Y_valid, X_test):    \r\n","    # (a) Modeling  \r\n","    model = XGBRegressor(objective=xgb_quantile_obj(quantile=q), seed=2021,\r\n","                         n_estimators=10000, bagging_fraction=0.7, learning_rate=0.1, subsample=0.7)                 \r\n","                         \r\n","    model.fit(X_train, Y_train, eval_metric = xgb_quantile_eval(quantile=q), \r\n","          eval_set=[(X_valid, Y_valid)], early_stopping_rounds=300, verbose=500)\r\n","\r\n","    # (b) Predictions\r\n","    pred = pd.Series(model.predict(X_test).round(2))\r\n","    return model, pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qcJAdTZQ5Dje"},"source":["# Target 예측\r\n","\r\n","def train_data_XGB(X_train, Y_train, X_valid, Y_valid, X_test):\r\n","\r\n","    XGB_models=[]\r\n","    XGB_actual_pred = pd.DataFrame()\r\n","\r\n","    for q in quantiles:\r\n","        print(q)\r\n","        model, pred = XGB(q, X_train, Y_train, X_valid, Y_valid, X_test)\r\n","        XGB_models.append(model)\r\n","        \r\n","        XGB_actual_pred = pd.concat([XGB_actual_pred,pred],axis=1)\r\n","\r\n","    XGB_actual_pred.columns=quantiles\r\n","    \r\n","    return XGB_models, XGB_actual_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D_iIcYxo5DkN"},"source":["# Target1\r\n","XGB_1, XGB_results_1 = train_data_XGB(X_train_1, Y_train_1, X_valid_1, Y_valid_1, X_test)\r\n","XGB_2, XGB_results_2 = train_data_XGB(X_train_2, Y_train_2, X_valid_2, Y_valid_2, X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KOv4nrvU5nDa"},"source":["# MLP"]},{"cell_type":"code","metadata":{"id":"LRo6FTev5k89"},"source":["import tensorflow as tf\r\n","from tensorflow.keras.backend import mean, maximum\r\n","\r\n","def quantile_loss(q, y, pred):\r\n","  err = (y-pred)\r\n","  return mean(maximum(q*err, (q-1)*err), axis=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3rV7urfF5k__"},"source":["from keras.callbacks import EarlyStopping\r\n","\r\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, restore_best_weights=True, patience=20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c_8C8nFVXn81"},"source":["LGBM_predict_1 = pd.read_csv('/content/drive/MyDrive/Dacon/LGBM_predict_1.csv')\r\n","LGBM_predict_2 = pd.read_csv('/content/drive/MyDrive/Dacon/LGBM_predict_2.csv')\r\n","LGBM_test_result_1 = pd.read_csv('/content/drive/MyDrive/Dacon/LGBM_test_result_1.csv')\r\n","LGBM_test_result_2 = pd.read_csv('/content/drive/MyDrive/Dacon/LGBM_test_result_2.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YnEzDCPG50XZ"},"source":["LGBM_predict_1 = LGBM_predict_1.round(2)\r\n","LGBM_predict_2 = LGBM_predict_2.round(2)\r\n","LGBM_test_result_1 = LGBM_test_result_1.round(2)\r\n","LGBM_test_result_2 = LGBM_test_result_2.round(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8uJMBZGh66k2"},"source":["XGB_test_result = pd.read_csv('/content/drive/MyDrive/Dacon/xgb_2.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3O6hN_CeJHpD"},"source":["XGB_test_result_1 = XGB_test_result[XGB_test_result['id'].apply(lambda x : True if 'Day7' in x else False)]\r\n","XGB_test_result_2 = XGB_test_result[XGB_test_result['id'].apply(lambda x : True if 'Day8' in x else False)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vXgD4MNffZDV"},"source":["import pickle\r\n","\r\n","with open(\"/content/drive/MyDrive/Dacon/preds_xgb_2.pickle\",\"rb\") as fr:\r\n","    XGB_predict_1, XGB_predict_2 = pickle.load(fr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0SuUJnnAW7zy"},"source":["XGB_test_result_1 = XGB_test_result_1.iloc[:, 1:]\r\n","XGB_test_result_2 = XGB_test_result_2.iloc[:, 1:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bkSmuN5pKl7p"},"source":["XGB_test_result_1 = XGB_test_result_1.reset_index().iloc[:, 1:]\r\n","XGB_test_result_2 = XGB_test_result_2.reset_index().iloc[:, 1:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"izbI2nA7tYRm"},"source":["XGB_test_result_1.columns = ['0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9']\r\n","XGB_test_result_2.columns = ['0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9']\r\n","\r\n","XGB_predict_1.columns = ['0.9', '0.8', '0.7', '0.6', '0.5', '0.4', '0.3', '0.2', '0.1']\r\n","XGB_predict_2.columns = ['0.9', '0.8', '0.7', '0.6', '0.5', '0.4', '0.3', '0.2', '0.1']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Pga7EAgYJkx"},"source":["XGB_predict_1 = XGB_predict_1.round(2)\r\n","XGB_predict_2 = XGB_predict_2.round(2)\r\n","\r\n","for i in range(9) : \r\n","  XGB_predict_1.iloc[:, i] = XGB_predict_1.iloc[:, i].apply(lambda x : 0 if x <=0 else x)\r\n","  XGB_predict_2.iloc[:, i] = XGB_predict_2.iloc[:, i].apply(lambda x : 0 if x <=0 else x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TnSPQYAU5lit"},"source":["def Stacking(LGBM_predict, LGBM_test_result, XGB_predict, XGB_test_result, Y_train, X_test):\r\n","  ST_pred = pd.DataFrame()\r\n","  ST_models = []\r\n","\r\n","  for q in quantiles :\r\n","    print(\"★★★★★★★★★★  \", q, \"  ★★★★★★★★★★\")\r\n","    pred1 = LGBM_predict[str(q)]\r\n","    pred2 = XGB_predict[str(q)]\r\n","\r\n","    MLP_train_x = np.array([pred1, pred2])\r\n","    MLP_train_x = MLP_train_x.transpose()\r\n","    MLP_train_x = MLP_train_x.reshape(1086, 48, 2)\r\n","\r\n","#==============================================================================================================================================\r\n","    MLP_model = tf.keras.Sequential([\r\n","                                # tf.keras.layers.Dense(units=6, activation='tanh', input_shape=(1,)),\r\n","                                tf.keras.layers.Dense(units=48, input_shape=(48, 2)),\r\n","                                tf.keras.layers.Dense(units=24),\r\n","                                tf.keras.layers.Dense(units=12),\r\n","                                tf.keras.layers.Dense(units=6),\r\n","                                tf.keras.layers.Flatten(),\r\n","                                tf.keras.layers.Dense(units=100),                            \r\n","                                tf.keras.layers.Dense(units=48, activation='relu'),\r\n","    ])\r\n","\r\n","    MLP_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0007), loss=lambda y,pred: quantile_loss(q,y,pred))\r\n","    #MLP_model.summary()\r\n","    \r\n","    MLP_model.fit(MLP_train_x, Y_train, epochs=1000, batch_size=1086, validation_split=0.25, callbacks=[es])\r\n","\r\n"," #==================================================================   \r\n","    pred3 = LGBM_test_result[str(q)]\r\n","    pred4 = XGB_test_result[str(q)]\r\n","\r\n","    MLP_test_x = np.array([pred3, pred4])\r\n","    MLP_test_x = MLP_test_x.transpose()\r\n","    MLP_test_x = MLP_test_x.reshape(81, 48, 2)\r\n","\r\n","    pred = MLP_model.predict(MLP_test_x)\r\n","    pred = pd.Series(pred.flatten())\r\n"," #==================================================================   \r\n","    ST_pred = pd.concat([ST_pred, pred],axis=1)\r\n","    ST_models.append(MLP_model)\r\n","\r\n","  ST_pred.columns=quantiles\r\n","\r\n","  return ST_models, ST_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1oBBm_x4sL0z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610196011725,"user_tz":-540,"elapsed":40690,"user":{"displayName":"정성헌","photoUrl":"","userId":"12794350723891602651"}},"outputId":"04853b4f-a446-4fa1-b797-fecb71660207"},"source":["MLP_models_1, MLP_result_1 = Stacking(LGBM_predict_1[192:52320], LGBM_test_result_1, XGB_predict_1[:-39], XGB_test_result_1, y_train_1[192:52320], X_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["★★★★★★★★★★   0.1   ★★★★★★★★★★\n","Epoch 1/1000\n","1/1 [==============================] - 1s 712ms/step - loss: 2.4865 - val_loss: 2.2804\n","Epoch 2/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 2.0580 - val_loss: 1.9582\n","Epoch 3/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.7243 - val_loss: 1.7081\n","Epoch 4/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.4677 - val_loss: 1.5231\n","Epoch 5/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.2750 - val_loss: 1.4047\n","Epoch 6/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 1.1479 - val_loss: 1.3378\n","Epoch 7/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.0726 - val_loss: 1.2973\n","Epoch 8/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 1.0279 - val_loss: 1.2684\n","Epoch 9/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 0.9959 - val_loss: 1.2477\n","Epoch 10/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9714 - val_loss: 1.2345\n","Epoch 11/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 0.9542 - val_loss: 1.2264\n","Epoch 12/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9427 - val_loss: 1.2205\n","Epoch 13/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 0.9343 - val_loss: 1.2165\n","Epoch 14/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9283 - val_loss: 1.2143\n","Epoch 15/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 0.9244 - val_loss: 1.2131\n","Epoch 16/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 0.9223 - val_loss: 1.2125\n","Epoch 17/1000\n","1/1 [==============================] - 0s 235ms/step - loss: 0.9209 - val_loss: 1.2121\n","Epoch 18/1000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.9200 - val_loss: 1.2116\n","Epoch 19/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 0.9192 - val_loss: 1.2112\n","Epoch 20/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9185 - val_loss: 1.2109\n","Epoch 21/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9180 - val_loss: 1.2107\n","Epoch 22/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9177 - val_loss: 1.2106\n","Epoch 23/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 0.9174 - val_loss: 1.2105\n","Epoch 24/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9173 - val_loss: 1.2104\n","Epoch 25/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9172 - val_loss: 1.2104\n","Epoch 26/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9172 - val_loss: 1.2103\n","Epoch 27/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9171 - val_loss: 1.2103\n","Epoch 28/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9171 - val_loss: 1.2103\n","Epoch 29/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 0.9171 - val_loss: 1.2103\n","Epoch 30/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9171 - val_loss: 1.2103\n","Epoch 31/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9171 - val_loss: 1.2103\n","Epoch 32/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9171 - val_loss: 1.2103\n","Epoch 33/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9171 - val_loss: 1.2103\n","Epoch 34/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9171 - val_loss: 1.2102\n","Epoch 35/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 0.9170 - val_loss: 1.2102\n","Epoch 36/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 0.9170 - val_loss: 1.2102\n","Epoch 37/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 0.9170 - val_loss: 1.2102\n","Epoch 38/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 0.9170 - val_loss: 1.2102\n","Epoch 39/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 0.9170 - val_loss: 1.2102\n","Epoch 40/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 0.9170 - val_loss: 1.2102\n","Epoch 41/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 0.9170 - val_loss: 1.2102\n","Epoch 42/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9170 - val_loss: 1.2102\n","Epoch 43/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9170 - val_loss: 1.2102\n","Epoch 44/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9170 - val_loss: 1.2102\n","Epoch 45/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9170 - val_loss: 1.2102\n","Epoch 46/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9170 - val_loss: 1.2102\n","Epoch 47/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 0.9170 - val_loss: 1.2102\n","Epoch 48/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9170 - val_loss: 1.2102\n","Epoch 49/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9169 - val_loss: 1.2102\n","Epoch 50/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 0.9169 - val_loss: 1.2102\n","Epoch 51/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 0.9169 - val_loss: 1.2102\n","Epoch 52/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9169 - val_loss: 1.2102\n","Epoch 53/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 0.9169 - val_loss: 1.2102\n","Epoch 54/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9169 - val_loss: 1.2102\n","Epoch 55/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 0.9169 - val_loss: 1.2102\n","Epoch 56/1000\n","1/1 [==============================] - 0s 218ms/step - loss: 0.9169 - val_loss: 1.2102\n","Epoch 57/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 0.9169 - val_loss: 1.2102\n","Epoch 58/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9169 - val_loss: 1.2102\n","Epoch 59/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9169 - val_loss: 1.2102\n","Epoch 60/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9169 - val_loss: 1.2102\n","Restoring model weights from the end of the best epoch.\n","Epoch 00060: early stopping\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0ba23d57b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","★★★★★★★★★★   0.2   ★★★★★★★★★★\n","Epoch 1/1000\n","1/1 [==============================] - 1s 710ms/step - loss: 3.1559 - val_loss: 3.2392\n","Epoch 2/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 2.6881 - val_loss: 2.9240\n","Epoch 3/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 2.3612 - val_loss: 2.7299\n","Epoch 4/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 2.1547 - val_loss: 2.6207\n","Epoch 5/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 2.0360 - val_loss: 2.5524\n","Epoch 6/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.9655 - val_loss: 2.5085\n","Epoch 7/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.9216 - val_loss: 2.4810\n","Epoch 8/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8934 - val_loss: 2.4624\n","Epoch 9/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8751 - val_loss: 2.4488\n","Epoch 10/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8623 - val_loss: 2.4396\n","Epoch 11/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8535 - val_loss: 2.4335\n","Epoch 12/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 1.8472 - val_loss: 2.4293\n","Epoch 13/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8427 - val_loss: 2.4263\n","Epoch 14/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 1.8397 - val_loss: 2.4241\n","Epoch 15/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8376 - val_loss: 2.4227\n","Epoch 16/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8362 - val_loss: 2.4218\n","Epoch 17/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8353 - val_loss: 2.4213\n","Epoch 18/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8347 - val_loss: 2.4209\n","Epoch 19/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8344 - val_loss: 2.4207\n","Epoch 20/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8342 - val_loss: 2.4206\n","Epoch 21/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8341 - val_loss: 2.4206\n","Epoch 22/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8340 - val_loss: 2.4206\n","Epoch 23/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8339 - val_loss: 2.4206\n","Epoch 24/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 1.8339 - val_loss: 2.4205\n","Epoch 25/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8339 - val_loss: 2.4205\n","Epoch 26/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8339 - val_loss: 2.4205\n","Epoch 27/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 28/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 29/1000\n","1/1 [==============================] - 0s 217ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 30/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 31/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 32/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 33/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 34/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 35/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 36/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 37/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 38/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 39/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 40/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 41/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 42/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 43/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 44/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 45/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 46/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 47/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 48/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 49/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 50/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 51/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 52/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 53/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 54/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 55/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 56/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 57/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 58/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 59/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 60/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 61/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 62/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 63/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 64/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 65/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 66/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 67/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 68/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 69/1000\n","1/1 [==============================] - 0s 232ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 70/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 71/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 1.8338 - val_loss: 2.4205\n","Epoch 72/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8338 - val_loss: 2.4205\n","Restoring model weights from the end of the best epoch.\n","Epoch 00072: early stopping\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0ba1946d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","★★★★★★★★★★   0.3   ★★★★★★★★★★\n","Epoch 1/1000\n","1/1 [==============================] - 1s 710ms/step - loss: 4.5788 - val_loss: 4.9096\n","Epoch 2/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 3.9793 - val_loss: 4.4639\n","Epoch 3/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.5484 - val_loss: 4.1482\n","Epoch 4/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.2445 - val_loss: 3.9429\n","Epoch 5/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.0454 - val_loss: 3.8217\n","Epoch 6/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 2.9315 - val_loss: 3.7536\n","Epoch 7/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 2.8665 - val_loss: 3.7151\n","Epoch 8/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 2.8296 - val_loss: 3.6948\n","Epoch 9/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 2.8110 - val_loss: 3.6800\n","Epoch 10/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 2.7972 - val_loss: 3.6657\n","Epoch 11/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 2.7846 - val_loss: 3.6534\n","Epoch 12/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 2.7735 - val_loss: 3.6441\n","Epoch 13/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 2.7654 - val_loss: 3.6381\n","Epoch 14/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 2.7601 - val_loss: 3.6348\n","Epoch 15/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 2.7567 - val_loss: 3.6331\n","Epoch 16/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 2.7547 - val_loss: 3.6325\n","Epoch 17/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 2.7534 - val_loss: 3.6321\n","Epoch 18/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 2.7526 - val_loss: 3.6318\n","Epoch 19/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 2.7521 - val_loss: 3.6315\n","Epoch 20/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 2.7517 - val_loss: 3.6313\n","Epoch 21/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 2.7515 - val_loss: 3.6311\n","Epoch 22/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 2.7513 - val_loss: 3.6310\n","Epoch 23/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 2.7512 - val_loss: 3.6309\n","Epoch 24/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 2.7511 - val_loss: 3.6309\n","Epoch 25/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 2.7510 - val_loss: 3.6308\n","Epoch 26/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 2.7510 - val_loss: 3.6308\n","Epoch 27/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 2.7510 - val_loss: 3.6308\n","Epoch 28/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 2.7509 - val_loss: 3.6308\n","Epoch 29/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 2.7509 - val_loss: 3.6308\n","Epoch 30/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 2.7508 - val_loss: 3.6308\n","Epoch 31/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 2.7508 - val_loss: 3.6308\n","Epoch 32/1000\n","1/1 [==============================] - 0s 224ms/step - loss: 2.7508 - val_loss: 3.6307\n","Epoch 33/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 2.7508 - val_loss: 3.6308\n","Epoch 34/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 2.7508 - val_loss: 3.6308\n","Epoch 35/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 2.7508 - val_loss: 3.6308\n","Epoch 36/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 2.7508 - val_loss: 3.6308\n","Epoch 37/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 2.7508 - val_loss: 3.6308\n","Epoch 38/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 2.7507 - val_loss: 3.6308\n","Epoch 39/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 2.7507 - val_loss: 3.6308\n","Epoch 40/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 2.7507 - val_loss: 3.6308\n","Epoch 41/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 2.7507 - val_loss: 3.6308\n","Epoch 42/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 2.7507 - val_loss: 3.6307\n","Epoch 43/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 2.7507 - val_loss: 3.6307\n","Epoch 44/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 2.7507 - val_loss: 3.6307\n","Epoch 45/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 2.7507 - val_loss: 3.6307\n","Epoch 46/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 2.7507 - val_loss: 3.6307\n","Epoch 47/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 2.7507 - val_loss: 3.6307\n","Epoch 48/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 2.7507 - val_loss: 3.6308\n","Epoch 49/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 2.7507 - val_loss: 3.6308\n","Epoch 50/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 2.7507 - val_loss: 3.6308\n","Epoch 51/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 2.7507 - val_loss: 3.6308\n","Epoch 52/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 2.7507 - val_loss: 3.6308\n","Epoch 53/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 2.7507 - val_loss: 3.6308\n","Epoch 54/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 2.7507 - val_loss: 3.6308\n","Epoch 55/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 2.7507 - val_loss: 3.6308\n","Epoch 56/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 2.7507 - val_loss: 3.6308\n","Epoch 57/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 2.7507 - val_loss: 3.6308\n","Epoch 58/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 2.7507 - val_loss: 3.6308\n","Epoch 59/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 2.7507 - val_loss: 3.6308\n","Epoch 60/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 2.7507 - val_loss: 3.6308\n","Epoch 61/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 2.7507 - val_loss: 3.6308\n","Epoch 62/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 2.7507 - val_loss: 3.6308\n","Epoch 63/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 2.7507 - val_loss: 3.6308\n","Epoch 64/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 2.7507 - val_loss: 3.6308\n","Epoch 65/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 2.7507 - val_loss: 3.6308\n","Restoring model weights from the end of the best epoch.\n","Epoch 00065: early stopping\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0b97bcaa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","★★★★★★★★★★   0.4   ★★★★★★★★★★\n","Epoch 1/1000\n","1/1 [==============================] - 1s 722ms/step - loss: 5.8565 - val_loss: 6.3133\n","Epoch 2/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.1412 - val_loss: 5.8333\n","Epoch 3/1000\n","1/1 [==============================] - 0s 226ms/step - loss: 4.6563 - val_loss: 5.4951\n","Epoch 4/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 4.3182 - val_loss: 5.2461\n","Epoch 5/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.0725 - val_loss: 5.0665\n","Epoch 6/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.8941 - val_loss: 4.9475\n","Epoch 7/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 3.7741 - val_loss: 4.8811\n","Epoch 8/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.7088 - val_loss: 4.8590\n","Epoch 9/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6860 - val_loss: 4.8509\n","Epoch 10/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 3.6794 - val_loss: 4.8471\n","Epoch 11/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 3.6762 - val_loss: 4.8456\n","Epoch 12/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 3.6743 - val_loss: 4.8448\n","Epoch 13/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 3.6727 - val_loss: 4.8441\n","Epoch 14/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 3.6713 - val_loss: 4.8432\n","Epoch 15/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6701 - val_loss: 4.8426\n","Epoch 16/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6692 - val_loss: 4.8422\n","Epoch 17/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6686 - val_loss: 4.8419\n","Epoch 18/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 3.6683 - val_loss: 4.8416\n","Epoch 19/1000\n","1/1 [==============================] - 0s 45ms/step - loss: 3.6681 - val_loss: 4.8414\n","Epoch 20/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6680 - val_loss: 4.8414\n","Epoch 21/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6679 - val_loss: 4.8413\n","Epoch 22/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6678 - val_loss: 4.8412\n","Epoch 23/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 3.6678 - val_loss: 4.8412\n","Epoch 24/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 3.6678 - val_loss: 4.8411\n","Epoch 25/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 3.6677 - val_loss: 4.8411\n","Epoch 26/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 3.6677 - val_loss: 4.8411\n","Epoch 27/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6677 - val_loss: 4.8411\n","Epoch 28/1000\n","1/1 [==============================] - 0s 45ms/step - loss: 3.6677 - val_loss: 4.8411\n","Epoch 29/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 3.6677 - val_loss: 4.8410\n","Epoch 30/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6676 - val_loss: 4.8410\n","Epoch 31/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6676 - val_loss: 4.8410\n","Epoch 32/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6676 - val_loss: 4.8410\n","Epoch 33/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6676 - val_loss: 4.8410\n","Epoch 34/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 3.6676 - val_loss: 4.8410\n","Epoch 35/1000\n","1/1 [==============================] - 0s 45ms/step - loss: 3.6676 - val_loss: 4.8411\n","Epoch 36/1000\n","1/1 [==============================] - 0s 45ms/step - loss: 3.6676 - val_loss: 4.8411\n","Epoch 37/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6676 - val_loss: 4.8411\n","Epoch 38/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 3.6676 - val_loss: 4.8411\n","Epoch 39/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 3.6676 - val_loss: 4.8411\n","Epoch 40/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6676 - val_loss: 4.8411\n","Epoch 41/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 3.6676 - val_loss: 4.8411\n","Epoch 42/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 3.6676 - val_loss: 4.8411\n","Epoch 43/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6676 - val_loss: 4.8411\n","Epoch 44/1000\n","1/1 [==============================] - 0s 238ms/step - loss: 3.6676 - val_loss: 4.8411\n","Epoch 45/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 3.6676 - val_loss: 4.8411\n","Epoch 46/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6676 - val_loss: 4.8411\n","Epoch 47/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6676 - val_loss: 4.8411\n","Epoch 48/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 3.6676 - val_loss: 4.8411\n","Epoch 49/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 3.6676 - val_loss: 4.8411\n","Epoch 50/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 3.6676 - val_loss: 4.8411\n","Restoring model weights from the end of the best epoch.\n","Epoch 00050: early stopping\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0b8f436950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","★★★★★★★★★★   0.5   ★★★★★★★★★★\n","Epoch 1/1000\n","1/1 [==============================] - 1s 734ms/step - loss: 5.4236 - val_loss: 6.5199\n","Epoch 2/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.0533 - val_loss: 6.2819\n","Epoch 3/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.8278 - val_loss: 6.1573\n","Epoch 4/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.7050 - val_loss: 6.0986\n","Epoch 5/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.6459 - val_loss: 6.0736\n","Epoch 6/1000\n","1/1 [==============================] - 0s 47ms/step - loss: 4.6191 - val_loss: 6.0619\n","Epoch 7/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.6073 - val_loss: 6.0570\n","Epoch 8/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 4.6013 - val_loss: 6.0543\n","Epoch 9/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 4.5978 - val_loss: 6.0531\n","Epoch 10/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 4.5952 - val_loss: 6.0525\n","Epoch 11/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 4.5928 - val_loss: 6.0518\n","Epoch 12/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.5908 - val_loss: 6.0518\n","Epoch 13/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.5894 - val_loss: 6.0518\n","Epoch 14/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.5885 - val_loss: 6.0520\n","Epoch 15/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.5878 - val_loss: 6.0520\n","Epoch 16/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.5871 - val_loss: 6.0519\n","Epoch 17/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 4.5864 - val_loss: 6.0518\n","Epoch 18/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.5858 - val_loss: 6.0517\n","Epoch 19/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.5854 - val_loss: 6.0516\n","Epoch 20/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 4.5852 - val_loss: 6.0514\n","Epoch 21/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 4.5850 - val_loss: 6.0513\n","Epoch 22/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 4.5849 - val_loss: 6.0513\n","Epoch 23/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.5849 - val_loss: 6.0512\n","Epoch 24/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.5848 - val_loss: 6.0512\n","Epoch 25/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.5848 - val_loss: 6.0512\n","Epoch 26/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 4.5847 - val_loss: 6.0512\n","Epoch 27/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.5847 - val_loss: 6.0511\n","Epoch 28/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.5847 - val_loss: 6.0511\n","Epoch 29/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 4.5846 - val_loss: 6.0511\n","Epoch 30/1000\n","1/1 [==============================] - 0s 238ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 31/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 32/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 33/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 34/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 35/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 36/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 37/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 38/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 39/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 40/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 41/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 42/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 43/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 44/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 45/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 46/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 47/1000\n","1/1 [==============================] - 0s 46ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 48/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 49/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 50/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 51/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 52/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.5846 - val_loss: 6.0510\n","Epoch 53/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 4.5846 - val_loss: 6.0510\n","Restoring model weights from the end of the best epoch.\n","Epoch 00053: early stopping\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0ba25451e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","★★★★★★★★★★   0.6   ★★★★★★★★★★\n","Epoch 1/1000\n","1/1 [==============================] - 1s 702ms/step - loss: 6.0791 - val_loss: 7.5000\n","Epoch 2/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.8149 - val_loss: 7.3656\n","Epoch 3/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.6604 - val_loss: 7.2999\n","Epoch 4/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.5752 - val_loss: 7.2740\n","Epoch 5/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.5347 - val_loss: 7.2635\n","Epoch 6/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.5173 - val_loss: 7.2607\n","Epoch 7/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 5.5111 - val_loss: 7.2610\n","Epoch 8/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.5087 - val_loss: 7.2619\n","Epoch 9/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 5.5072 - val_loss: 7.2620\n","Epoch 10/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.5061 - val_loss: 7.2613\n","Epoch 11/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.5052 - val_loss: 7.2607\n","Epoch 12/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.5044 - val_loss: 7.2606\n","Epoch 13/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.5036 - val_loss: 7.2608\n","Epoch 14/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.5028 - val_loss: 7.2609\n","Epoch 15/1000\n","1/1 [==============================] - 0s 231ms/step - loss: 5.5022 - val_loss: 7.2607\n","Epoch 16/1000\n","1/1 [==============================] - 0s 45ms/step - loss: 5.5019 - val_loss: 7.2607\n","Epoch 17/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.5017 - val_loss: 7.2607\n","Epoch 18/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.5015 - val_loss: 7.2608\n","Epoch 19/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.5014 - val_loss: 7.2609\n","Epoch 20/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.5014 - val_loss: 7.2609\n","Epoch 21/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.5014 - val_loss: 7.2609\n","Epoch 22/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.5013 - val_loss: 7.2609\n","Epoch 23/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.5013 - val_loss: 7.2609\n","Epoch 24/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.5013 - val_loss: 7.2610\n","Epoch 25/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.5012 - val_loss: 7.2610\n","Epoch 26/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 5.5012 - val_loss: 7.2611\n","Epoch 27/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.5011 - val_loss: 7.2611\n","Epoch 28/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.5011 - val_loss: 7.2611\n","Epoch 29/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.5011 - val_loss: 7.2610\n","Epoch 30/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.5011 - val_loss: 7.2610\n","Epoch 31/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 5.5010 - val_loss: 7.2609\n","Epoch 32/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.5009 - val_loss: 7.2608\n","Restoring model weights from the end of the best epoch.\n","Epoch 00032: early stopping\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0ba3770730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","★★★★★★★★★★   0.7   ★★★★★★★★★★\n","Epoch 1/1000\n","1/1 [==============================] - 1s 683ms/step - loss: 6.9672 - val_loss: 8.3783\n","Epoch 2/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.4920 - val_loss: 8.2169\n","Epoch 3/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.2877 - val_loss: 8.2060\n","Epoch 4/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 6.2429 - val_loss: 8.2424\n","Epoch 5/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.2590 - val_loss: 8.2641\n","Epoch 6/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.2751 - val_loss: 8.2714\n","Epoch 7/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.2810 - val_loss: 8.2718\n","Epoch 8/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 6.2803 - val_loss: 8.2666\n","Epoch 9/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.2740 - val_loss: 8.2570\n","Epoch 10/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.2651 - val_loss: 8.2461\n","Epoch 11/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 6.2560 - val_loss: 8.2359\n","Epoch 12/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 6.2475 - val_loss: 8.2262\n","Epoch 13/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 6.2405 - val_loss: 8.2169\n","Epoch 14/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.2350 - val_loss: 8.2087\n","Epoch 15/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.2306 - val_loss: 8.2020\n","Epoch 16/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 6.2271 - val_loss: 8.1962\n","Epoch 17/1000\n","1/1 [==============================] - 0s 45ms/step - loss: 6.2240 - val_loss: 8.1912\n","Epoch 18/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.2210 - val_loss: 8.1863\n","Epoch 19/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 6.2177 - val_loss: 8.1818\n","Epoch 20/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 6.2138 - val_loss: 8.1778\n","Epoch 21/1000\n","1/1 [==============================] - 0s 248ms/step - loss: 6.2099 - val_loss: 8.1743\n","Epoch 22/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 6.2064 - val_loss: 8.1715\n","Epoch 23/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.2035 - val_loss: 8.1693\n","Epoch 24/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.2012 - val_loss: 8.1676\n","Epoch 25/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.1995 - val_loss: 8.1665\n","Epoch 26/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 6.1983 - val_loss: 8.1660\n","Epoch 27/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.1975 - val_loss: 8.1660\n","Epoch 28/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.1968 - val_loss: 8.1665\n","Epoch 29/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.1963 - val_loss: 8.1674\n","Epoch 30/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.1960 - val_loss: 8.1686\n","Epoch 31/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 6.1957 - val_loss: 8.1698\n","Epoch 32/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.1954 - val_loss: 8.1708\n","Epoch 33/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.1952 - val_loss: 8.1717\n","Epoch 34/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.1950 - val_loss: 8.1723\n","Epoch 35/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.1947 - val_loss: 8.1726\n","Epoch 36/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 6.1944 - val_loss: 8.1725\n","Epoch 37/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 6.1940 - val_loss: 8.1721\n","Epoch 38/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 6.1937 - val_loss: 8.1717\n","Epoch 39/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.1933 - val_loss: 8.1712\n","Epoch 40/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.1930 - val_loss: 8.1707\n","Epoch 41/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.1926 - val_loss: 8.1705\n","Epoch 42/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.1923 - val_loss: 8.1706\n","Epoch 43/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.1919 - val_loss: 8.1710\n","Epoch 44/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 6.1916 - val_loss: 8.1716\n","Epoch 45/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.1912 - val_loss: 8.1723\n","Epoch 46/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.1909 - val_loss: 8.1730\n","Restoring model weights from the end of the best epoch.\n","Epoch 00046: early stopping\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0b7c8d2730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","★★★★★★★★★★   0.8   ★★★★★★★★★★\n","Epoch 1/1000\n","1/1 [==============================] - 1s 698ms/step - loss: 7.2986 - val_loss: 8.8067\n","Epoch 2/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.8317 - val_loss: 8.5960\n","Epoch 3/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.5848 - val_loss: 8.5282\n","Epoch 4/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 6.4958 - val_loss: 8.5106\n","Epoch 5/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 6.4643 - val_loss: 8.4882\n","Epoch 6/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.4421 - val_loss: 8.4488\n","Epoch 7/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.4137 - val_loss: 8.3957\n","Epoch 8/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 6.3779 - val_loss: 8.3366\n","Epoch 9/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.3430 - val_loss: 8.2800\n","Epoch 10/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 6.3114 - val_loss: 8.2256\n","Epoch 11/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 6.2783 - val_loss: 8.1737\n","Epoch 12/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.2410 - val_loss: 8.1257\n","Epoch 13/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 6.2036 - val_loss: 8.0839\n","Epoch 14/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.1718 - val_loss: 8.0518\n","Epoch 15/1000\n","1/1 [==============================] - 0s 241ms/step - loss: 6.1467 - val_loss: 8.0246\n","Epoch 16/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.1258 - val_loss: 8.0022\n","Epoch 17/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 6.1078 - val_loss: 7.9850\n","Epoch 18/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.0922 - val_loss: 7.9689\n","Epoch 19/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 6.0786 - val_loss: 7.9541\n","Epoch 20/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.0666 - val_loss: 7.9384\n","Epoch 21/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.0552 - val_loss: 7.9226\n","Epoch 22/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 6.0447 - val_loss: 7.9076\n","Epoch 23/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.0353 - val_loss: 7.8955\n","Epoch 24/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 6.0286 - val_loss: 7.8891\n","Epoch 25/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.0239 - val_loss: 7.8880\n","Epoch 26/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 6.0197 - val_loss: 7.8921\n","Epoch 27/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 6.0161 - val_loss: 7.9000\n","Epoch 28/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.0133 - val_loss: 7.9087\n","Epoch 29/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.0115 - val_loss: 7.9149\n","Epoch 30/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.0102 - val_loss: 7.9155\n","Epoch 31/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.0084 - val_loss: 7.9101\n","Epoch 32/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.0060 - val_loss: 7.9014\n","Epoch 33/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.0037 - val_loss: 7.8928\n","Epoch 34/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.0022 - val_loss: 7.8868\n","Epoch 35/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 6.0008 - val_loss: 7.8855\n","Epoch 36/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 5.9987 - val_loss: 7.8881\n","Epoch 37/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.9963 - val_loss: 7.8927\n","Epoch 38/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.9942 - val_loss: 7.8968\n","Epoch 39/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.9927 - val_loss: 7.8975\n","Epoch 40/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.9914 - val_loss: 7.8942\n","Epoch 41/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.9899 - val_loss: 7.8877\n","Epoch 42/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.9884 - val_loss: 7.8807\n","Epoch 43/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.9872 - val_loss: 7.8764\n","Epoch 44/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 5.9863 - val_loss: 7.8760\n","Epoch 45/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.9853 - val_loss: 7.8788\n","Epoch 46/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.9844 - val_loss: 7.8833\n","Epoch 47/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.9836 - val_loss: 7.8866\n","Epoch 48/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.9831 - val_loss: 7.8872\n","Epoch 49/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.9826 - val_loss: 7.8843\n","Epoch 50/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.9820 - val_loss: 7.8797\n","Epoch 51/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.9815 - val_loss: 7.8756\n","Epoch 52/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.9811 - val_loss: 7.8739\n","Epoch 53/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.9807 - val_loss: 7.8750\n","Epoch 54/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.9802 - val_loss: 7.8775\n","Epoch 55/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.9796 - val_loss: 7.8799\n","Epoch 56/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.9792 - val_loss: 7.8796\n","Epoch 57/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.9788 - val_loss: 7.8766\n","Epoch 58/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.9783 - val_loss: 7.8717\n","Epoch 59/1000\n","1/1 [==============================] - 0s 238ms/step - loss: 5.9778 - val_loss: 7.8672\n","Epoch 60/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.9774 - val_loss: 7.8647\n","Epoch 61/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.9770 - val_loss: 7.8648\n","Epoch 62/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.9765 - val_loss: 7.8660\n","Epoch 63/1000\n","1/1 [==============================] - 0s 45ms/step - loss: 5.9761 - val_loss: 7.8662\n","Epoch 64/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.9757 - val_loss: 7.8647\n","Epoch 65/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.9754 - val_loss: 7.8613\n","Epoch 66/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.9750 - val_loss: 7.8574\n","Epoch 67/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.9747 - val_loss: 7.8544\n","Epoch 68/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 5.9744 - val_loss: 7.8528\n","Epoch 69/1000\n","1/1 [==============================] - 0s 45ms/step - loss: 5.9741 - val_loss: 7.8521\n","Epoch 70/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.9738 - val_loss: 7.8517\n","Epoch 71/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.9735 - val_loss: 7.8500\n","Epoch 72/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.9732 - val_loss: 7.8481\n","Epoch 73/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.9729 - val_loss: 7.8463\n","Epoch 74/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.9726 - val_loss: 7.8458\n","Epoch 75/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 5.9724 - val_loss: 7.8460\n","Epoch 76/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.9721 - val_loss: 7.8450\n","Epoch 77/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.9718 - val_loss: 7.8433\n","Epoch 78/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.9716 - val_loss: 7.8420\n","Epoch 79/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 5.9714 - val_loss: 7.8420\n","Epoch 80/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.9711 - val_loss: 7.8428\n","Epoch 81/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 5.9709 - val_loss: 7.8426\n","Epoch 82/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.9707 - val_loss: 7.8411\n","Epoch 83/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.9705 - val_loss: 7.8398\n","Epoch 84/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.9702 - val_loss: 7.8392\n","Epoch 85/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.9700 - val_loss: 7.8391\n","Epoch 86/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 5.9698 - val_loss: 7.8385\n","Epoch 87/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.9696 - val_loss: 7.8379\n","Epoch 88/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 5.9693 - val_loss: 7.8375\n","Epoch 89/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 5.9691 - val_loss: 7.8367\n","Epoch 90/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.9689 - val_loss: 7.8367\n","Epoch 91/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.9687 - val_loss: 7.8365\n","Epoch 92/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.9685 - val_loss: 7.8361\n","Epoch 93/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.9683 - val_loss: 7.8353\n","Epoch 94/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.9681 - val_loss: 7.8348\n","Epoch 95/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.9679 - val_loss: 7.8351\n","Epoch 96/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.9677 - val_loss: 7.8343\n","Epoch 97/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.9675 - val_loss: 7.8342\n","Epoch 98/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.9673 - val_loss: 7.8346\n","Epoch 99/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.9671 - val_loss: 7.8347\n","Epoch 100/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.9669 - val_loss: 7.8335\n","Epoch 101/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.9667 - val_loss: 7.8337\n","Epoch 102/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.9666 - val_loss: 7.8346\n","Epoch 103/1000\n","1/1 [==============================] - 0s 243ms/step - loss: 5.9664 - val_loss: 7.8351\n","Epoch 104/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.9662 - val_loss: 7.8346\n","Epoch 105/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.9660 - val_loss: 7.8346\n","Epoch 106/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.9658 - val_loss: 7.8353\n","Epoch 107/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.9656 - val_loss: 7.8358\n","Epoch 108/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.9654 - val_loss: 7.8354\n","Epoch 109/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.9653 - val_loss: 7.8357\n","Epoch 110/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.9651 - val_loss: 7.8370\n","Epoch 111/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.9649 - val_loss: 7.8365\n","Epoch 112/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.9647 - val_loss: 7.8369\n","Epoch 113/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.9645 - val_loss: 7.8384\n","Epoch 114/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.9643 - val_loss: 7.8379\n","Epoch 115/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.9641 - val_loss: 7.8387\n","Epoch 116/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.9639 - val_loss: 7.8386\n","Epoch 117/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.9638 - val_loss: 7.8393\n","Epoch 118/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.9636 - val_loss: 7.8399\n","Epoch 119/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.9634 - val_loss: 7.8401\n","Epoch 120/1000\n","1/1 [==============================] - 0s 46ms/step - loss: 5.9632 - val_loss: 7.8408\n","Restoring model weights from the end of the best epoch.\n","Epoch 00120: early stopping\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0b8f396598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","★★★★★★★★★★   0.9   ★★★★★★★★★★\n","Epoch 1/1000\n","1/1 [==============================] - 1s 692ms/step - loss: 6.6030 - val_loss: 8.1864\n","Epoch 2/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 6.1608 - val_loss: 7.8232\n","Epoch 3/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.9194 - val_loss: 7.6303\n","Epoch 4/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.8163 - val_loss: 7.5262\n","Epoch 5/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.7664 - val_loss: 7.3986\n","Epoch 6/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.6746 - val_loss: 7.2495\n","Epoch 7/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.5539 - val_loss: 7.1819\n","Epoch 8/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.4798 - val_loss: 7.1786\n","Epoch 9/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.4604 - val_loss: 7.1923\n","Epoch 10/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.4643 - val_loss: 7.1963\n","Epoch 11/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.4658 - val_loss: 7.1809\n","Epoch 12/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.4533 - val_loss: 7.1474\n","Epoch 13/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.4254 - val_loss: 7.1041\n","Epoch 14/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.3882 - val_loss: 7.0578\n","Epoch 15/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 5.3509 - val_loss: 7.0185\n","Epoch 16/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.3192 - val_loss: 6.9857\n","Epoch 17/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.2966 - val_loss: 6.9600\n","Epoch 18/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 5.2817 - val_loss: 6.9374\n","Epoch 19/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.2709 - val_loss: 6.9212\n","Epoch 20/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.2650 - val_loss: 6.9129\n","Epoch 21/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 5.2615 - val_loss: 6.9078\n","Epoch 22/1000\n","1/1 [==============================] - 0s 247ms/step - loss: 5.2585 - val_loss: 6.9029\n","Epoch 23/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.2554 - val_loss: 6.8976\n","Epoch 24/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 5.2519 - val_loss: 6.8923\n","Epoch 25/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.2484 - val_loss: 6.8848\n","Epoch 26/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 5.2446 - val_loss: 6.8749\n","Epoch 27/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.2401 - val_loss: 6.8631\n","Epoch 28/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.2357 - val_loss: 6.8493\n","Epoch 29/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.2308 - val_loss: 6.8350\n","Epoch 30/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.2259 - val_loss: 6.8229\n","Epoch 31/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.2212 - val_loss: 6.8140\n","Epoch 32/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.2167 - val_loss: 6.8073\n","Epoch 33/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.2128 - val_loss: 6.8013\n","Epoch 34/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.2094 - val_loss: 6.7942\n","Epoch 35/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.2060 - val_loss: 6.7855\n","Epoch 36/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.2025 - val_loss: 6.7760\n","Epoch 37/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1990 - val_loss: 6.7659\n","Epoch 38/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.1955 - val_loss: 6.7560\n","Epoch 39/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1922 - val_loss: 6.7470\n","Epoch 40/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1892 - val_loss: 6.7380\n","Epoch 41/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1865 - val_loss: 6.7287\n","Epoch 42/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1842 - val_loss: 6.7196\n","Epoch 43/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1824 - val_loss: 6.7110\n","Epoch 44/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1810 - val_loss: 6.7031\n","Epoch 45/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1797 - val_loss: 6.6962\n","Epoch 46/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1782 - val_loss: 6.6903\n","Epoch 47/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1763 - val_loss: 6.6852\n","Epoch 48/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1743 - val_loss: 6.6800\n","Epoch 49/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 5.1722 - val_loss: 6.6736\n","Epoch 50/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1701 - val_loss: 6.6656\n","Epoch 51/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.1679 - val_loss: 6.6567\n","Epoch 52/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.1658 - val_loss: 6.6480\n","Epoch 53/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.1639 - val_loss: 6.6387\n","Epoch 54/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1623 - val_loss: 6.6289\n","Epoch 55/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.1610 - val_loss: 6.6191\n","Epoch 56/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1599 - val_loss: 6.6101\n","Epoch 57/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1589 - val_loss: 6.6031\n","Epoch 58/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.1580 - val_loss: 6.5987\n","Epoch 59/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1570 - val_loss: 6.5951\n","Epoch 60/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.1560 - val_loss: 6.5908\n","Epoch 61/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.1550 - val_loss: 6.5844\n","Epoch 62/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.1541 - val_loss: 6.5768\n","Epoch 63/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1532 - val_loss: 6.5686\n","Epoch 64/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.1524 - val_loss: 6.5611\n","Epoch 65/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.1516 - val_loss: 6.5552\n","Epoch 66/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.1508 - val_loss: 6.5517\n","Epoch 67/1000\n","1/1 [==============================] - 0s 243ms/step - loss: 5.1501 - val_loss: 6.5494\n","Epoch 68/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.1496 - val_loss: 6.5462\n","Epoch 69/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.1491 - val_loss: 6.5421\n","Epoch 70/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.1485 - val_loss: 6.5375\n","Epoch 71/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.1480 - val_loss: 6.5321\n","Epoch 72/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1475 - val_loss: 6.5266\n","Epoch 73/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 5.1471 - val_loss: 6.5229\n","Epoch 74/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.1467 - val_loss: 6.5202\n","Epoch 75/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 5.1462 - val_loss: 6.5170\n","Epoch 76/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.1458 - val_loss: 6.5134\n","Epoch 77/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1455 - val_loss: 6.5100\n","Epoch 78/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 5.1452 - val_loss: 6.5079\n","Epoch 79/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.1448 - val_loss: 6.5067\n","Epoch 80/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.1443 - val_loss: 6.5054\n","Epoch 81/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1440 - val_loss: 6.5030\n","Epoch 82/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.1436 - val_loss: 6.5004\n","Epoch 83/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1433 - val_loss: 6.4992\n","Epoch 84/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.1431 - val_loss: 6.4997\n","Epoch 85/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1427 - val_loss: 6.5013\n","Epoch 86/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1424 - val_loss: 6.5007\n","Epoch 87/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.1422 - val_loss: 6.4987\n","Epoch 88/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1419 - val_loss: 6.4973\n","Epoch 89/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1415 - val_loss: 6.4976\n","Epoch 90/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 5.1413 - val_loss: 6.4979\n","Epoch 91/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.1410 - val_loss: 6.4983\n","Epoch 92/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.1407 - val_loss: 6.4979\n","Epoch 93/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1405 - val_loss: 6.4959\n","Epoch 94/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1403 - val_loss: 6.4938\n","Epoch 95/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1400 - val_loss: 6.4930\n","Epoch 96/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.1398 - val_loss: 6.4935\n","Epoch 97/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1396 - val_loss: 6.4944\n","Epoch 98/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1393 - val_loss: 6.4948\n","Epoch 99/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1391 - val_loss: 6.4950\n","Epoch 100/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.1388 - val_loss: 6.4946\n","Epoch 101/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.1386 - val_loss: 6.4946\n","Epoch 102/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.1384 - val_loss: 6.4936\n","Epoch 103/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 5.1382 - val_loss: 6.4938\n","Epoch 104/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1379 - val_loss: 6.4949\n","Epoch 105/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1377 - val_loss: 6.4954\n","Epoch 106/1000\n","1/1 [==============================] - 0s 47ms/step - loss: 5.1375 - val_loss: 6.4940\n","Epoch 107/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.1373 - val_loss: 6.4928\n","Epoch 108/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1371 - val_loss: 6.4925\n","Epoch 109/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.1369 - val_loss: 6.4927\n","Epoch 110/1000\n","1/1 [==============================] - 0s 46ms/step - loss: 5.1366 - val_loss: 6.4927\n","Epoch 111/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.1364 - val_loss: 6.4920\n","Epoch 112/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1362 - val_loss: 6.4906\n","Epoch 113/1000\n","1/1 [==============================] - 0s 249ms/step - loss: 5.1360 - val_loss: 6.4906\n","Epoch 114/1000\n","1/1 [==============================] - 0s 45ms/step - loss: 5.1358 - val_loss: 6.4909\n","Epoch 115/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.1356 - val_loss: 6.4907\n","Epoch 116/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1353 - val_loss: 6.4903\n","Epoch 117/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1351 - val_loss: 6.4894\n","Epoch 118/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.1349 - val_loss: 6.4892\n","Epoch 119/1000\n","1/1 [==============================] - 0s 46ms/step - loss: 5.1347 - val_loss: 6.4898\n","Epoch 120/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1345 - val_loss: 6.4908\n","Epoch 121/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1343 - val_loss: 6.4909\n","Epoch 122/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 5.1341 - val_loss: 6.4898\n","Epoch 123/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.1339 - val_loss: 6.4891\n","Epoch 124/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1337 - val_loss: 6.4903\n","Epoch 125/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.1335 - val_loss: 6.4915\n","Epoch 126/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.1332 - val_loss: 6.4905\n","Epoch 127/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1330 - val_loss: 6.4895\n","Epoch 128/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1328 - val_loss: 6.4906\n","Epoch 129/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.1326 - val_loss: 6.4915\n","Epoch 130/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 5.1324 - val_loss: 6.4906\n","Epoch 131/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1322 - val_loss: 6.4904\n","Epoch 132/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 5.1320 - val_loss: 6.4916\n","Epoch 133/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 5.1318 - val_loss: 6.4923\n","Epoch 134/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.1316 - val_loss: 6.4907\n","Epoch 135/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.1314 - val_loss: 6.4904\n","Epoch 136/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1312 - val_loss: 6.4924\n","Epoch 137/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1310 - val_loss: 6.4934\n","Epoch 138/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1307 - val_loss: 6.4912\n","Epoch 139/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1306 - val_loss: 6.4925\n","Epoch 140/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 5.1303 - val_loss: 6.4939\n","Epoch 141/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1301 - val_loss: 6.4925\n","Epoch 142/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.1299 - val_loss: 6.4920\n","Epoch 143/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.1297 - val_loss: 6.4939\n","Restoring model weights from the end of the best epoch.\n","Epoch 00143: early stopping\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0b7ca161e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kbeMH2MfyH3L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610196210101,"user_tz":-540,"elapsed":43234,"user":{"displayName":"정성헌","photoUrl":"","userId":"12794350723891602651"}},"outputId":"52a512b4-3275-40b4-c490-0d51185232bc"},"source":["MLP_models_2, MLP_result_2 = Stacking(LGBM_predict_2[240-48:52320], LGBM_test_result_2, XGB_predict_2[:-39], XGB_test_result_2, y_train_2[192:52320], X_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["★★★★★★★★★★   0.1   ★★★★★★★★★★\n","Epoch 1/1000\n","1/1 [==============================] - 1s 685ms/step - loss: 1.8396 - val_loss: 1.7959\n","Epoch 2/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 1.5484 - val_loss: 1.5825\n","Epoch 3/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.3323 - val_loss: 1.4333\n","Epoch 4/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.1823 - val_loss: 1.3311\n","Epoch 5/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 1.0772 - val_loss: 1.2719\n","Epoch 6/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 1.0127 - val_loss: 1.2389\n","Epoch 7/1000\n","1/1 [==============================] - 0s 35ms/step - loss: 0.9743 - val_loss: 1.2180\n","Epoch 8/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9490 - val_loss: 1.2034\n","Epoch 9/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9336 - val_loss: 1.1968\n","Epoch 10/1000\n","1/1 [==============================] - 0s 435ms/step - loss: 0.9274 - val_loss: 1.1934\n","Epoch 11/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9250 - val_loss: 1.1913\n","Epoch 12/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 0.9237 - val_loss: 1.1901\n","Epoch 13/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 0.9230 - val_loss: 1.1895\n","Epoch 14/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 0.9224 - val_loss: 1.1891\n","Epoch 15/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 0.9218 - val_loss: 1.1889\n","Epoch 16/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 0.9214 - val_loss: 1.1887\n","Epoch 17/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 0.9211 - val_loss: 1.1886\n","Epoch 18/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9209 - val_loss: 1.1886\n","Epoch 19/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 0.9208 - val_loss: 1.1885\n","Epoch 20/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 0.9207 - val_loss: 1.1885\n","Epoch 21/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9207 - val_loss: 1.1885\n","Epoch 22/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 23/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 24/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 25/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 26/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 27/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 28/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 29/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 30/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 31/1000\n","1/1 [==============================] - 0s 35ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 32/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 33/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 34/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 35/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 36/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 37/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 38/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 39/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 40/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 41/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 42/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 43/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 44/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 0.9206 - val_loss: 1.1885\n","Epoch 45/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 46/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 47/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 48/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 49/1000\n","1/1 [==============================] - 0s 215ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 50/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 51/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 52/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 53/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 54/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 55/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 56/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 57/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 58/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 59/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 60/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 61/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 62/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 63/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 64/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 65/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 66/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 67/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 68/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 69/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 70/1000\n","1/1 [==============================] - 0s 35ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 71/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 72/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 73/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 74/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 75/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 76/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 77/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 78/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 79/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 80/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 81/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 82/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 83/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 84/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 85/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 86/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 87/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 88/1000\n","1/1 [==============================] - 0s 215ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 89/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 90/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 91/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 92/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 93/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 94/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 95/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 96/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 97/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 98/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 99/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 100/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 101/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 102/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 103/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 104/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 105/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 106/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 107/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 108/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 109/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 110/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 111/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 112/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 113/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 114/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 115/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 116/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 117/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 118/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 119/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 120/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 121/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1884\n","Epoch 122/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9206 - val_loss: 1.1884\n","Restoring model weights from the end of the best epoch.\n","Epoch 00122: early stopping\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0ba9d76a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","★★★★★★★★★★   0.2   ★★★★★★★★★★\n","Epoch 1/1000\n","1/1 [==============================] - 1s 903ms/step - loss: 3.3683 - val_loss: 3.3945\n","Epoch 2/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 2.9328 - val_loss: 3.0786\n","Epoch 3/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 2.6021 - val_loss: 2.8558\n","Epoch 4/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 2.3647 - val_loss: 2.7032\n","Epoch 5/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 2.1993 - val_loss: 2.5956\n","Epoch 6/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 2.0836 - val_loss: 2.5236\n","Epoch 7/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 2.0048 - val_loss: 2.4750\n","Epoch 8/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.9510 - val_loss: 2.4400\n","Epoch 9/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 1.9124 - val_loss: 2.4160\n","Epoch 10/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8856 - val_loss: 2.4006\n","Epoch 11/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8692 - val_loss: 2.3921\n","Epoch 12/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8597 - val_loss: 2.3874\n","Epoch 13/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 1.8536 - val_loss: 2.3844\n","Epoch 14/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8498 - val_loss: 2.3823\n","Epoch 15/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8473 - val_loss: 2.3808\n","Epoch 16/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8455 - val_loss: 2.3797\n","Epoch 17/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 1.8443 - val_loss: 2.3789\n","Epoch 18/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8433 - val_loss: 2.3783\n","Epoch 19/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8426 - val_loss: 2.3779\n","Epoch 20/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 1.8422 - val_loss: 2.3776\n","Epoch 21/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8419 - val_loss: 2.3774\n","Epoch 22/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8417 - val_loss: 2.3773\n","Epoch 23/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8416 - val_loss: 2.3772\n","Epoch 24/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8415 - val_loss: 2.3771\n","Epoch 25/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8414 - val_loss: 2.3770\n","Epoch 26/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8414 - val_loss: 2.3770\n","Epoch 27/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 1.8413 - val_loss: 2.3769\n","Epoch 28/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8413 - val_loss: 2.3769\n","Epoch 29/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8413 - val_loss: 2.3769\n","Epoch 30/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 1.8413 - val_loss: 2.3768\n","Epoch 31/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 1.8413 - val_loss: 2.3768\n","Epoch 32/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 1.8413 - val_loss: 2.3768\n","Epoch 33/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8413 - val_loss: 2.3768\n","Epoch 34/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 35/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 36/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 37/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 38/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 39/1000\n","1/1 [==============================] - 0s 220ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 40/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 41/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 42/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 43/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 44/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 45/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 46/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 47/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 48/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 49/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 50/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 51/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 52/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 53/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 54/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 55/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 56/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 57/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 58/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 59/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 60/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 61/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 62/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 63/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 1.8412 - val_loss: 2.3769\n","Epoch 64/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8412 - val_loss: 2.3769\n","Epoch 65/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8412 - val_loss: 2.3769\n","Epoch 66/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8412 - val_loss: 2.3769\n","Epoch 67/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 1.8412 - val_loss: 2.3769\n","Epoch 68/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 1.8412 - val_loss: 2.3768\n","Epoch 69/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8411 - val_loss: 2.3768\n","Epoch 70/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 1.8411 - val_loss: 2.3768\n","Epoch 71/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 1.8411 - val_loss: 2.3768\n","Epoch 72/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 1.8411 - val_loss: 2.3768\n","Epoch 73/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8411 - val_loss: 2.3768\n","Epoch 74/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 1.8411 - val_loss: 2.3769\n","Epoch 75/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 1.8411 - val_loss: 2.3769\n","Epoch 76/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 1.8411 - val_loss: 2.3769\n","Epoch 77/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 1.8411 - val_loss: 2.3769\n","Restoring model weights from the end of the best epoch.\n","Epoch 00077: early stopping\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0ba27b2c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","★★★★★★★★★★   0.3   ★★★★★★★★★★\n","Epoch 1/1000\n","1/1 [==============================] - 1s 897ms/step - loss: 5.4673 - val_loss: 5.4108\n","Epoch 2/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 4.5794 - val_loss: 4.7482\n","Epoch 3/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.9327 - val_loss: 4.3122\n","Epoch 4/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 3.5095 - val_loss: 4.0276\n","Epoch 5/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 3.2295 - val_loss: 3.8296\n","Epoch 6/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 3.0362 - val_loss: 3.7032\n","Epoch 7/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 2.9127 - val_loss: 3.6234\n","Epoch 8/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 2.8367 - val_loss: 3.5901\n","Epoch 9/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 2.8014 - val_loss: 3.5790\n","Epoch 10/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 2.7894 - val_loss: 3.5729\n","Epoch 11/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 2.7834 - val_loss: 3.5690\n","Epoch 12/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 2.7799 - val_loss: 3.5669\n","Epoch 13/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 2.7782 - val_loss: 3.5662\n","Epoch 14/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 2.7775 - val_loss: 3.5659\n","Epoch 15/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 2.7770 - val_loss: 3.5657\n","Epoch 16/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 2.7765 - val_loss: 3.5655\n","Epoch 17/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 2.7758 - val_loss: 3.5654\n","Epoch 18/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 2.7751 - val_loss: 3.5652\n","Epoch 19/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 2.7743 - val_loss: 3.5651\n","Epoch 20/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 2.7734 - val_loss: 3.5651\n","Epoch 21/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 2.7725 - val_loss: 3.5651\n","Epoch 22/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 2.7717 - val_loss: 3.5652\n","Epoch 23/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 2.7708 - val_loss: 3.5653\n","Epoch 24/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 2.7700 - val_loss: 3.5654\n","Epoch 25/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 2.7693 - val_loss: 3.5655\n","Epoch 26/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 2.7686 - val_loss: 3.5657\n","Epoch 27/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 2.7679 - val_loss: 3.5658\n","Epoch 28/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 2.7673 - val_loss: 3.5659\n","Epoch 29/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 2.7667 - val_loss: 3.5660\n","Epoch 30/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 2.7662 - val_loss: 3.5661\n","Epoch 31/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 2.7657 - val_loss: 3.5662\n","Epoch 32/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 2.7652 - val_loss: 3.5662\n","Epoch 33/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 2.7648 - val_loss: 3.5663\n","Epoch 34/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 2.7645 - val_loss: 3.5663\n","Epoch 35/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 2.7642 - val_loss: 3.5663\n","Epoch 36/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 2.7639 - val_loss: 3.5664\n","Epoch 37/1000\n","1/1 [==============================] - 0s 228ms/step - loss: 2.7637 - val_loss: 3.5664\n","Epoch 38/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 2.7635 - val_loss: 3.5664\n","Epoch 39/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 2.7633 - val_loss: 3.5664\n","Epoch 40/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 2.7631 - val_loss: 3.5664\n","Restoring model weights from the end of the best epoch.\n","Epoch 00040: early stopping\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0b8f362f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","★★★★★★★★★★   0.4   ★★★★★★★★★★\n","Epoch 1/1000\n","1/1 [==============================] - 1s 695ms/step - loss: 4.9749 - val_loss: 5.4897\n","Epoch 2/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.4655 - val_loss: 5.1516\n","Epoch 3/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 4.1126 - val_loss: 4.9624\n","Epoch 4/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.9104 - val_loss: 4.8579\n","Epoch 5/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 3.8076 - val_loss: 4.8210\n","Epoch 6/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.7703 - val_loss: 4.7994\n","Epoch 7/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 3.7452 - val_loss: 4.7770\n","Epoch 8/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 3.7206 - val_loss: 4.7632\n","Epoch 9/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 3.7034 - val_loss: 4.7571\n","Epoch 10/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6928 - val_loss: 4.7549\n","Epoch 11/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6873 - val_loss: 4.7542\n","Epoch 12/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6849 - val_loss: 4.7539\n","Epoch 13/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 3.6837 - val_loss: 4.7539\n","Epoch 14/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6831 - val_loss: 4.7538\n","Epoch 15/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6829 - val_loss: 4.7539\n","Epoch 16/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6828 - val_loss: 4.7539\n","Epoch 17/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6828 - val_loss: 4.7539\n","Epoch 18/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6828 - val_loss: 4.7539\n","Epoch 19/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 3.6827 - val_loss: 4.7539\n","Epoch 20/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 3.6827 - val_loss: 4.7539\n","Epoch 21/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 3.6827 - val_loss: 4.7538\n","Epoch 22/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6826 - val_loss: 4.7538\n","Epoch 23/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6826 - val_loss: 4.7538\n","Epoch 24/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 3.6826 - val_loss: 4.7537\n","Epoch 25/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 3.6825 - val_loss: 4.7537\n","Epoch 26/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6825 - val_loss: 4.7537\n","Epoch 27/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6825 - val_loss: 4.7537\n","Epoch 28/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 3.6825 - val_loss: 4.7537\n","Epoch 29/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 30/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 31/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 32/1000\n","1/1 [==============================] - 0s 229ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 33/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 34/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 35/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 36/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 37/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 38/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 39/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 40/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 41/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 42/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 43/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 44/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 45/1000\n","1/1 [==============================] - 0s 46ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 46/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 47/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 48/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 49/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 50/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 51/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 52/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 53/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 54/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 55/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 56/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 57/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 58/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 59/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 60/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 61/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 62/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 63/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 64/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 65/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 66/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 67/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 68/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 69/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 70/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 71/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 72/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 73/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 74/1000\n","1/1 [==============================] - 0s 225ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 75/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 76/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 77/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 78/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 79/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 80/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 81/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 82/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 83/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 84/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 85/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 86/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 87/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 3.6824 - val_loss: 4.7537\n","Epoch 88/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 3.6824 - val_loss: 4.7537\n","Restoring model weights from the end of the best epoch.\n","Epoch 00088: early stopping\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0bcc0a7268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","★★★★★★★★★★   0.5   ★★★★★★★★★★\n","Epoch 1/1000\n","1/1 [==============================] - 1s 717ms/step - loss: 5.8657 - val_loss: 6.5754\n","Epoch 2/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.3160 - val_loss: 6.2610\n","Epoch 3/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.9746 - val_loss: 6.0906\n","Epoch 4/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 4.7883 - val_loss: 6.0089\n","Epoch 5/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 4.6914 - val_loss: 5.9721\n","Epoch 6/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.6461 - val_loss: 5.9540\n","Epoch 7/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 4.6243 - val_loss: 5.9468\n","Epoch 8/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.6147 - val_loss: 5.9443\n","Epoch 9/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 4.6104 - val_loss: 5.9435\n","Epoch 10/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.6079 - val_loss: 5.9434\n","Epoch 11/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.6063 - val_loss: 5.9435\n","Epoch 12/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.6054 - val_loss: 5.9436\n","Epoch 13/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.6049 - val_loss: 5.9438\n","Epoch 14/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.6047 - val_loss: 5.9440\n","Epoch 15/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.6045 - val_loss: 5.9440\n","Epoch 16/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 4.6044 - val_loss: 5.9440\n","Epoch 17/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 4.6043 - val_loss: 5.9440\n","Epoch 18/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 4.6042 - val_loss: 5.9439\n","Epoch 19/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.6041 - val_loss: 5.9438\n","Epoch 20/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 4.6040 - val_loss: 5.9436\n","Epoch 21/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.6038 - val_loss: 5.9434\n","Epoch 22/1000\n","1/1 [==============================] - 0s 237ms/step - loss: 4.6037 - val_loss: 5.9432\n","Epoch 23/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 4.6036 - val_loss: 5.9430\n","Epoch 24/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.6035 - val_loss: 5.9428\n","Epoch 25/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 4.6034 - val_loss: 5.9427\n","Epoch 26/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.6033 - val_loss: 5.9426\n","Epoch 27/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.6033 - val_loss: 5.9425\n","Epoch 28/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.6032 - val_loss: 5.9424\n","Epoch 29/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 4.6032 - val_loss: 5.9423\n","Epoch 30/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.6031 - val_loss: 5.9422\n","Epoch 31/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 4.6031 - val_loss: 5.9422\n","Epoch 32/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.6031 - val_loss: 5.9422\n","Epoch 33/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.6031 - val_loss: 5.9421\n","Epoch 34/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 4.6030 - val_loss: 5.9421\n","Epoch 35/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 4.6030 - val_loss: 5.9421\n","Epoch 36/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.6030 - val_loss: 5.9421\n","Epoch 37/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.6030 - val_loss: 5.9421\n","Epoch 38/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 4.6030 - val_loss: 5.9421\n","Epoch 39/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.6030 - val_loss: 5.9420\n","Epoch 40/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.6030 - val_loss: 5.9420\n","Epoch 41/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.6030 - val_loss: 5.9420\n","Epoch 42/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 4.6030 - val_loss: 5.9420\n","Epoch 43/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 4.6030 - val_loss: 5.9420\n","Epoch 44/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 45/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 46/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 47/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 48/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 49/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 50/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 51/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 52/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 53/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 54/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 55/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 56/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 57/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 58/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 59/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 60/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 61/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 62/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 63/1000\n","1/1 [==============================] - 0s 234ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 64/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 65/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 66/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 67/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 68/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 69/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 70/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 71/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 72/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 73/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 74/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 75/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 76/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 77/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 78/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 79/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 4.6029 - val_loss: 5.9420\n","Epoch 80/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 4.6029 - val_loss: 5.9420\n","Restoring model weights from the end of the best epoch.\n","Epoch 00080: early stopping\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0b97bca950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","★★★★★★★★★★   0.6   ★★★★★★★★★★\n","Epoch 1/1000\n","1/1 [==============================] - 1s 699ms/step - loss: 6.8254 - val_loss: 7.7785\n","Epoch 2/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.3666 - val_loss: 7.4971\n","Epoch 3/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.0453 - val_loss: 7.3242\n","Epoch 4/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.8299 - val_loss: 7.2239\n","Epoch 5/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.6941 - val_loss: 7.1711\n","Epoch 6/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.6138 - val_loss: 7.1459\n","Epoch 7/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.5733 - val_loss: 7.1392\n","Epoch 8/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.5553 - val_loss: 7.1378\n","Epoch 9/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.5451 - val_loss: 7.1369\n","Epoch 10/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.5394 - val_loss: 7.1373\n","Epoch 11/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.5360 - val_loss: 7.1369\n","Epoch 12/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.5337 - val_loss: 7.1365\n","Epoch 13/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.5316 - val_loss: 7.1353\n","Epoch 14/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.5296 - val_loss: 7.1341\n","Epoch 15/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.5279 - val_loss: 7.1334\n","Epoch 16/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.5266 - val_loss: 7.1326\n","Epoch 17/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.5254 - val_loss: 7.1318\n","Epoch 18/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.5245 - val_loss: 7.1315\n","Epoch 19/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.5241 - val_loss: 7.1309\n","Epoch 20/1000\n","1/1 [==============================] - 0s 242ms/step - loss: 5.5239 - val_loss: 7.1307\n","Epoch 21/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.5240 - val_loss: 7.1306\n","Epoch 22/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.5239 - val_loss: 7.1306\n","Epoch 23/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.5238 - val_loss: 7.1307\n","Epoch 24/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.5238 - val_loss: 7.1307\n","Epoch 25/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.5237 - val_loss: 7.1307\n","Epoch 26/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.5236 - val_loss: 7.1307\n","Epoch 27/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.5236 - val_loss: 7.1307\n","Epoch 28/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.5235 - val_loss: 7.1307\n","Epoch 29/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.5235 - val_loss: 7.1307\n","Epoch 30/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.5234 - val_loss: 7.1309\n","Epoch 31/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.5234 - val_loss: 7.1310\n","Epoch 32/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.5234 - val_loss: 7.1311\n","Epoch 33/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.5234 - val_loss: 7.1312\n","Epoch 34/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.5233 - val_loss: 7.1313\n","Epoch 35/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.5233 - val_loss: 7.1314\n","Epoch 36/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.5233 - val_loss: 7.1314\n","Epoch 37/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.5233 - val_loss: 7.1313\n","Epoch 38/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.5232 - val_loss: 7.1312\n","Epoch 39/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.5232 - val_loss: 7.1312\n","Epoch 40/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 5.5232 - val_loss: 7.1312\n","Epoch 41/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.5232 - val_loss: 7.1312\n","Restoring model weights from the end of the best epoch.\n","Epoch 00041: early stopping\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0b9cc747b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","★★★★★★★★★★   0.7   ★★★★★★★★★★\n","Epoch 1/1000\n","1/1 [==============================] - 1s 720ms/step - loss: 7.2013 - val_loss: 8.4501\n","Epoch 2/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 6.7990 - val_loss: 8.2544\n","Epoch 3/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.5488 - val_loss: 8.1518\n","Epoch 4/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 6.4073 - val_loss: 8.1004\n","Epoch 5/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.3365 - val_loss: 8.0838\n","Epoch 6/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.3080 - val_loss: 8.0841\n","Epoch 7/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.3016 - val_loss: 8.0796\n","Epoch 8/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 6.2989 - val_loss: 8.0679\n","Epoch 9/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.2913 - val_loss: 8.0532\n","Epoch 10/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.2842 - val_loss: 8.0435\n","Epoch 11/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.2793 - val_loss: 8.0388\n","Epoch 12/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 6.2767 - val_loss: 8.0361\n","Epoch 13/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.2747 - val_loss: 8.0335\n","Epoch 14/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.2720 - val_loss: 8.0308\n","Epoch 15/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.2680 - val_loss: 8.0277\n","Epoch 16/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.2633 - val_loss: 8.0259\n","Epoch 17/1000\n","1/1 [==============================] - 0s 247ms/step - loss: 6.2588 - val_loss: 8.0264\n","Epoch 18/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.2561 - val_loss: 8.0285\n","Epoch 19/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.2548 - val_loss: 8.0317\n","Epoch 20/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 6.2547 - val_loss: 8.0353\n","Epoch 21/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 6.2548 - val_loss: 8.0380\n","Epoch 22/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 6.2546 - val_loss: 8.0392\n","Epoch 23/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.2538 - val_loss: 8.0390\n","Epoch 24/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.2527 - val_loss: 8.0381\n","Epoch 25/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 6.2514 - val_loss: 8.0370\n","Epoch 26/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 6.2500 - val_loss: 8.0359\n","Epoch 27/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 6.2488 - val_loss: 8.0350\n","Epoch 28/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.2475 - val_loss: 8.0345\n","Epoch 29/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 6.2465 - val_loss: 8.0348\n","Epoch 30/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 6.2456 - val_loss: 8.0361\n","Epoch 31/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.2449 - val_loss: 8.0381\n","Epoch 32/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.2442 - val_loss: 8.0404\n","Epoch 33/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.2437 - val_loss: 8.0428\n","Epoch 34/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.2431 - val_loss: 8.0452\n","Epoch 35/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 6.2425 - val_loss: 8.0471\n","Epoch 36/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.2420 - val_loss: 8.0483\n","Restoring model weights from the end of the best epoch.\n","Epoch 00036: early stopping\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0ba29dc158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","★★★★★★★★★★   0.8   ★★★★★★★★★★\n","Epoch 1/1000\n","1/1 [==============================] - 1s 702ms/step - loss: 6.6740 - val_loss: 8.1749\n","Epoch 2/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.5315 - val_loss: 8.1103\n","Epoch 3/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 6.4692 - val_loss: 8.0747\n","Epoch 4/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.4401 - val_loss: 8.0408\n","Epoch 5/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 6.4222 - val_loss: 8.0102\n","Epoch 6/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 6.4108 - val_loss: 7.9891\n","Epoch 7/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.4058 - val_loss: 7.9760\n","Epoch 8/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.4024 - val_loss: 7.9697\n","Epoch 9/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.3984 - val_loss: 7.9612\n","Epoch 10/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 6.3880 - val_loss: 7.9568\n","Epoch 11/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.3764 - val_loss: 7.9549\n","Epoch 12/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 6.3655 - val_loss: 7.9508\n","Epoch 13/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 6.3548 - val_loss: 7.9415\n","Epoch 14/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.3439 - val_loss: 7.9226\n","Epoch 15/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 6.3329 - val_loss: 7.8951\n","Epoch 16/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.3214 - val_loss: 7.8706\n","Epoch 17/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 6.3144 - val_loss: 7.8516\n","Epoch 18/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 6.3102 - val_loss: 7.8412\n","Epoch 19/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.3067 - val_loss: 7.8384\n","Epoch 20/1000\n","1/1 [==============================] - 0s 239ms/step - loss: 6.3036 - val_loss: 7.8411\n","Epoch 21/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 6.3014 - val_loss: 7.8460\n","Epoch 22/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.3005 - val_loss: 7.8510\n","Epoch 23/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.2998 - val_loss: 7.8532\n","Epoch 24/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.2989 - val_loss: 7.8528\n","Epoch 25/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 6.2978 - val_loss: 7.8518\n","Epoch 26/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.2968 - val_loss: 7.8517\n","Epoch 27/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.2957 - val_loss: 7.8532\n","Epoch 28/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.2942 - val_loss: 7.8554\n","Epoch 29/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 6.2926 - val_loss: 7.8561\n","Epoch 30/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 6.2909 - val_loss: 7.8551\n","Epoch 31/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.2892 - val_loss: 7.8528\n","Epoch 32/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 6.2876 - val_loss: 7.8494\n","Epoch 33/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 6.2861 - val_loss: 7.8472\n","Epoch 34/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.2846 - val_loss: 7.8461\n","Epoch 35/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.2831 - val_loss: 7.8469\n","Epoch 36/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.2818 - val_loss: 7.8490\n","Epoch 37/1000\n","1/1 [==============================] - 0s 45ms/step - loss: 6.2805 - val_loss: 7.8512\n","Epoch 38/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 6.2793 - val_loss: 7.8525\n","Epoch 39/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 6.2783 - val_loss: 7.8523\n","Restoring model weights from the end of the best epoch.\n","Epoch 00039: early stopping\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0ba3785ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","★★★★★★★★★★   0.9   ★★★★★★★★★★\n","Epoch 1/1000\n","1/1 [==============================] - 1s 708ms/step - loss: 7.4156 - val_loss: 9.1528\n","Epoch 2/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 7.1115 - val_loss: 8.7420\n","Epoch 3/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 6.8244 - val_loss: 8.3851\n","Epoch 4/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.5921 - val_loss: 8.0958\n","Epoch 5/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.4263 - val_loss: 7.8713\n","Epoch 6/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.3107 - val_loss: 7.6884\n","Epoch 7/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.2169 - val_loss: 7.5424\n","Epoch 8/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 6.1397 - val_loss: 7.4278\n","Epoch 9/1000\n","1/1 [==============================] - 0s 46ms/step - loss: 6.0741 - val_loss: 7.3360\n","Epoch 10/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 6.0115 - val_loss: 7.2584\n","Epoch 11/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.9484 - val_loss: 7.1932\n","Epoch 12/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.8853 - val_loss: 7.1402\n","Epoch 13/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.8248 - val_loss: 7.1002\n","Epoch 14/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 5.7707 - val_loss: 7.0763\n","Epoch 15/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 5.7291 - val_loss: 7.0633\n","Epoch 16/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.6964 - val_loss: 7.0582\n","Epoch 17/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.6709 - val_loss: 7.0555\n","Epoch 18/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.6510 - val_loss: 7.0498\n","Epoch 19/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.6343 - val_loss: 7.0367\n","Epoch 20/1000\n","1/1 [==============================] - 0s 245ms/step - loss: 5.6194 - val_loss: 7.0137\n","Epoch 21/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.6055 - val_loss: 6.9824\n","Epoch 22/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.5918 - val_loss: 6.9459\n","Epoch 23/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.5799 - val_loss: 6.9105\n","Epoch 24/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.5722 - val_loss: 6.8806\n","Epoch 25/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.5680 - val_loss: 6.8574\n","Epoch 26/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.5659 - val_loss: 6.8396\n","Epoch 27/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.5627 - val_loss: 6.8259\n","Epoch 28/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.5553 - val_loss: 6.8159\n","Epoch 29/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.5429 - val_loss: 6.8086\n","Epoch 30/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.5271 - val_loss: 6.8019\n","Epoch 31/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.5086 - val_loss: 6.7936\n","Epoch 32/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.4893 - val_loss: 6.7838\n","Epoch 33/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.4707 - val_loss: 6.7697\n","Epoch 34/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.4540 - val_loss: 6.7525\n","Epoch 35/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.4408 - val_loss: 6.7338\n","Epoch 36/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.4317 - val_loss: 6.7165\n","Epoch 37/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.4258 - val_loss: 6.7033\n","Epoch 38/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.4223 - val_loss: 6.6949\n","Epoch 39/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.4199 - val_loss: 6.6902\n","Epoch 40/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.4171 - val_loss: 6.6902\n","Epoch 41/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.4138 - val_loss: 6.6942\n","Epoch 42/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.4101 - val_loss: 6.7002\n","Epoch 43/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.4061 - val_loss: 6.7052\n","Epoch 44/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.4021 - val_loss: 6.7069\n","Epoch 45/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.3978 - val_loss: 6.7036\n","Epoch 46/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.3930 - val_loss: 6.6956\n","Epoch 47/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.3882 - val_loss: 6.6830\n","Epoch 48/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.3839 - val_loss: 6.6682\n","Epoch 49/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.3799 - val_loss: 6.6545\n","Epoch 50/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.3760 - val_loss: 6.6445\n","Epoch 51/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.3724 - val_loss: 6.6400\n","Epoch 52/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.3687 - val_loss: 6.6410\n","Epoch 53/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.3647 - val_loss: 6.6460\n","Epoch 54/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.3609 - val_loss: 6.6521\n","Epoch 55/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.3573 - val_loss: 6.6560\n","Epoch 56/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.3541 - val_loss: 6.6547\n","Epoch 57/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.3507 - val_loss: 6.6478\n","Epoch 58/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.3473 - val_loss: 6.6377\n","Epoch 59/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.3441 - val_loss: 6.6274\n","Epoch 60/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.3411 - val_loss: 6.6192\n","Epoch 61/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.3383 - val_loss: 6.6135\n","Epoch 62/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.3354 - val_loss: 6.6102\n","Epoch 63/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.3325 - val_loss: 6.6093\n","Epoch 64/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.3296 - val_loss: 6.6101\n","Epoch 65/1000\n","1/1 [==============================] - 0s 247ms/step - loss: 5.3268 - val_loss: 6.6100\n","Epoch 66/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 5.3243 - val_loss: 6.6078\n","Epoch 67/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.3218 - val_loss: 6.6026\n","Epoch 68/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.3193 - val_loss: 6.5960\n","Epoch 69/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.3170 - val_loss: 6.5897\n","Epoch 70/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.3146 - val_loss: 6.5844\n","Epoch 71/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.3122 - val_loss: 6.5809\n","Epoch 72/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.3098 - val_loss: 6.5791\n","Epoch 73/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.3075 - val_loss: 6.5784\n","Epoch 74/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.3054 - val_loss: 6.5783\n","Epoch 75/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 5.3033 - val_loss: 6.5773\n","Epoch 76/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.3014 - val_loss: 6.5749\n","Epoch 77/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.2997 - val_loss: 6.5708\n","Epoch 78/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.2980 - val_loss: 6.5650\n","Epoch 79/1000\n","1/1 [==============================] - 0s 47ms/step - loss: 5.2966 - val_loss: 6.5605\n","Epoch 80/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 5.2952 - val_loss: 6.5585\n","Epoch 81/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.2939 - val_loss: 6.5585\n","Epoch 82/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.2927 - val_loss: 6.5587\n","Epoch 83/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.2916 - val_loss: 6.5565\n","Epoch 84/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 5.2904 - val_loss: 6.5522\n","Epoch 85/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.2893 - val_loss: 6.5480\n","Epoch 86/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.2883 - val_loss: 6.5458\n","Epoch 87/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.2873 - val_loss: 6.5450\n","Epoch 88/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.2864 - val_loss: 6.5449\n","Epoch 89/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.2854 - val_loss: 6.5448\n","Epoch 90/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.2844 - val_loss: 6.5442\n","Epoch 91/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.2834 - val_loss: 6.5434\n","Epoch 92/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.2824 - val_loss: 6.5435\n","Epoch 93/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.2813 - val_loss: 6.5422\n","Epoch 94/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.2804 - val_loss: 6.5394\n","Epoch 95/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.2794 - val_loss: 6.5361\n","Epoch 96/1000\n","1/1 [==============================] - 0s 36ms/step - loss: 5.2784 - val_loss: 6.5353\n","Epoch 97/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.2774 - val_loss: 6.5351\n","Epoch 98/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.2765 - val_loss: 6.5346\n","Epoch 99/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.2756 - val_loss: 6.5338\n","Epoch 100/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.2747 - val_loss: 6.5336\n","Epoch 101/1000\n","1/1 [==============================] - 0s 45ms/step - loss: 5.2737 - val_loss: 6.5342\n","Epoch 102/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.2728 - val_loss: 6.5344\n","Epoch 103/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.2720 - val_loss: 6.5346\n","Epoch 104/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.2712 - val_loss: 6.5335\n","Epoch 105/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.2704 - val_loss: 6.5314\n","Epoch 106/1000\n","1/1 [==============================] - 0s 37ms/step - loss: 5.2695 - val_loss: 6.5292\n","Epoch 107/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.2686 - val_loss: 6.5284\n","Epoch 108/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.2678 - val_loss: 6.5260\n","Epoch 109/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.2671 - val_loss: 6.5239\n","Epoch 110/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.2662 - val_loss: 6.5230\n","Epoch 111/1000\n","1/1 [==============================] - 0s 249ms/step - loss: 5.2654 - val_loss: 6.5227\n","Epoch 112/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.2646 - val_loss: 6.5212\n","Epoch 113/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 5.2639 - val_loss: 6.5214\n","Epoch 114/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.2631 - val_loss: 6.5221\n","Epoch 115/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.2622 - val_loss: 6.5225\n","Epoch 116/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.2614 - val_loss: 6.5220\n","Epoch 117/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.2606 - val_loss: 6.5194\n","Epoch 118/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.2598 - val_loss: 6.5176\n","Epoch 119/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.2590 - val_loss: 6.5193\n","Epoch 120/1000\n","1/1 [==============================] - 0s 45ms/step - loss: 5.2582 - val_loss: 6.5197\n","Epoch 121/1000\n","1/1 [==============================] - 0s 45ms/step - loss: 5.2574 - val_loss: 6.5172\n","Epoch 122/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.2566 - val_loss: 6.5149\n","Epoch 123/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.2558 - val_loss: 6.5141\n","Epoch 124/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.2549 - val_loss: 6.5146\n","Epoch 125/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 5.2540 - val_loss: 6.5138\n","Epoch 126/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.2532 - val_loss: 6.5124\n","Epoch 127/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.2523 - val_loss: 6.5134\n","Epoch 128/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.2514 - val_loss: 6.5135\n","Epoch 129/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.2505 - val_loss: 6.5140\n","Epoch 130/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.2496 - val_loss: 6.5140\n","Epoch 131/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.2486 - val_loss: 6.5125\n","Epoch 132/1000\n","1/1 [==============================] - 0s 47ms/step - loss: 5.2477 - val_loss: 6.5115\n","Epoch 133/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.2467 - val_loss: 6.5104\n","Epoch 134/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.2456 - val_loss: 6.5126\n","Epoch 135/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.2446 - val_loss: 6.5122\n","Epoch 136/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.2436 - val_loss: 6.5108\n","Epoch 137/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.2425 - val_loss: 6.5111\n","Epoch 138/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.2413 - val_loss: 6.5104\n","Epoch 139/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.2402 - val_loss: 6.5089\n","Epoch 140/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.2390 - val_loss: 6.5093\n","Epoch 141/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.2378 - val_loss: 6.5092\n","Epoch 142/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 5.2365 - val_loss: 6.5102\n","Epoch 143/1000\n","1/1 [==============================] - 0s 50ms/step - loss: 5.2353 - val_loss: 6.5114\n","Epoch 144/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.2340 - val_loss: 6.5109\n","Epoch 145/1000\n","1/1 [==============================] - 0s 45ms/step - loss: 5.2327 - val_loss: 6.5088\n","Epoch 146/1000\n","1/1 [==============================] - 0s 45ms/step - loss: 5.2313 - val_loss: 6.5085\n","Epoch 147/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.2298 - val_loss: 6.5089\n","Epoch 148/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.2283 - val_loss: 6.5058\n","Epoch 149/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.2269 - val_loss: 6.5073\n","Epoch 150/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.2253 - val_loss: 6.5071\n","Epoch 151/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.2237 - val_loss: 6.5064\n","Epoch 152/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.2220 - val_loss: 6.5071\n","Epoch 153/1000\n","1/1 [==============================] - 0s 45ms/step - loss: 5.2203 - val_loss: 6.5067\n","Epoch 154/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.2186 - val_loss: 6.5027\n","Epoch 155/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.2168 - val_loss: 6.5058\n","Epoch 156/1000\n","1/1 [==============================] - 0s 246ms/step - loss: 5.2150 - val_loss: 6.5062\n","Epoch 157/1000\n","1/1 [==============================] - 0s 48ms/step - loss: 5.2131 - val_loss: 6.5013\n","Epoch 158/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.2113 - val_loss: 6.5082\n","Epoch 159/1000\n","1/1 [==============================] - 0s 45ms/step - loss: 5.2093 - val_loss: 6.5054\n","Epoch 160/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.2075 - val_loss: 6.5060\n","Epoch 161/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 5.2055 - val_loss: 6.5123\n","Epoch 162/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.2036 - val_loss: 6.5021\n","Epoch 163/1000\n","1/1 [==============================] - 0s 41ms/step - loss: 5.2018 - val_loss: 6.5179\n","Epoch 164/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.2000 - val_loss: 6.5063\n","Epoch 165/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.1978 - val_loss: 6.5134\n","Epoch 166/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1958 - val_loss: 6.5085\n","Epoch 167/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.1939 - val_loss: 6.5178\n","Epoch 168/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.1918 - val_loss: 6.5205\n","Epoch 169/1000\n","1/1 [==============================] - 0s 45ms/step - loss: 5.1897 - val_loss: 6.5127\n","Epoch 170/1000\n","1/1 [==============================] - 0s 44ms/step - loss: 5.1880 - val_loss: 6.5295\n","Epoch 171/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.1859 - val_loss: 6.5244\n","Epoch 172/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.1837 - val_loss: 6.5243\n","Epoch 173/1000\n","1/1 [==============================] - 0s 42ms/step - loss: 5.1818 - val_loss: 6.5372\n","Epoch 174/1000\n","1/1 [==============================] - 0s 43ms/step - loss: 5.1802 - val_loss: 6.5211\n","Epoch 175/1000\n","1/1 [==============================] - 0s 38ms/step - loss: 5.1777 - val_loss: 6.5503\n","Epoch 176/1000\n","1/1 [==============================] - 0s 39ms/step - loss: 5.1760 - val_loss: 6.5259\n","Epoch 177/1000\n","1/1 [==============================] - 0s 40ms/step - loss: 5.1727 - val_loss: 6.5522\n","Restoring model weights from the end of the best epoch.\n","Epoch 00177: early stopping\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0ba984cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iroRXFSw5xEo"},"source":["# RESULT"]},{"cell_type":"code","metadata":{"id":"GKmxK3xK5ln8","colab":{"base_uri":"https://localhost:8080/","height":407},"executionInfo":{"status":"ok","timestamp":1610196220067,"user_tz":-540,"elapsed":608,"user":{"displayName":"정성헌","photoUrl":"","userId":"12794350723891602651"}},"outputId":"3abd03b9-c605-4992-e67c-ebe1a5ded154"},"source":["submission = pd.read_csv('/content/drive/MyDrive/Dacon/sample_submission.csv')\r\n","submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = MLP_result_1.sort_index().values\r\n","submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = MLP_result_2.sort_index().values\r\n","submission"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>q_0.1</th>\n","      <th>q_0.2</th>\n","      <th>q_0.3</th>\n","      <th>q_0.4</th>\n","      <th>q_0.5</th>\n","      <th>q_0.6</th>\n","      <th>q_0.7</th>\n","      <th>q_0.8</th>\n","      <th>q_0.9</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.csv_Day7_0h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>3.729900</td>\n","      <td>13.514359</td>\n","      <td>27.080038</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.csv_Day7_0h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>3.794032</td>\n","      <td>13.358665</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.csv_Day7_1h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>4.880234</td>\n","      <td>13.181517</td>\n","      <td>27.436176</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.csv_Day7_1h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>5.274422</td>\n","      <td>13.634405</td>\n","      <td>22.294998</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.csv_Day7_2h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>2.656944</td>\n","      <td>12.665893</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7771</th>\n","      <td>80.csv_Day8_21h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>14.875137</td>\n","      <td>27.642363</td>\n","      <td>43.029587</td>\n","    </tr>\n","    <tr>\n","      <th>7772</th>\n","      <td>80.csv_Day8_22h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>11.120885</td>\n","      <td>25.983728</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>7773</th>\n","      <td>80.csv_Day8_22h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>12.158747</td>\n","      <td>34.509766</td>\n","      <td>40.329529</td>\n","    </tr>\n","    <tr>\n","      <th>7774</th>\n","      <td>80.csv_Day8_23h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.414246</td>\n","      <td>0.000000</td>\n","      <td>26.027014</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>7775</th>\n","      <td>80.csv_Day8_23h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>10.156621</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7776 rows × 10 columns</p>\n","</div>"],"text/plain":["                      id  q_0.1  q_0.2  ...      q_0.7      q_0.8      q_0.9\n","0       0.csv_Day7_0h00m    0.0    0.0  ...   3.729900  13.514359  27.080038\n","1       0.csv_Day7_0h30m    0.0    0.0  ...   3.794032  13.358665   0.000000\n","2       0.csv_Day7_1h00m    0.0    0.0  ...   4.880234  13.181517  27.436176\n","3       0.csv_Day7_1h30m    0.0    0.0  ...   5.274422  13.634405  22.294998\n","4       0.csv_Day7_2h00m    0.0    0.0  ...   2.656944  12.665893   0.000000\n","...                  ...    ...    ...  ...        ...        ...        ...\n","7771  80.csv_Day8_21h30m    0.0    0.0  ...  14.875137  27.642363  43.029587\n","7772  80.csv_Day8_22h00m    0.0    0.0  ...  11.120885  25.983728   0.000000\n","7773  80.csv_Day8_22h30m    0.0    0.0  ...  12.158747  34.509766  40.329529\n","7774  80.csv_Day8_23h00m    0.0    0.0  ...   0.000000  26.027014   0.000000\n","7775  80.csv_Day8_23h30m    0.0    0.0  ...  10.156621   0.000000   0.000000\n","\n","[7776 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":384}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Xb94tYB0_U3l","executionInfo":{"status":"ok","timestamp":1610196245998,"user_tz":-540,"elapsed":714,"user":{"displayName":"정성헌","photoUrl":"","userId":"12794350723891602651"}},"outputId":"1585ff82-948a-48a6-c994-4ce7b5717d30"},"source":["submission[0:48]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>q_0.1</th>\n","      <th>q_0.2</th>\n","      <th>q_0.3</th>\n","      <th>q_0.4</th>\n","      <th>q_0.5</th>\n","      <th>q_0.6</th>\n","      <th>q_0.7</th>\n","      <th>q_0.8</th>\n","      <th>q_0.9</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.csv_Day7_0h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>3.729900</td>\n","      <td>13.514359</td>\n","      <td>27.080038</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.csv_Day7_0h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>3.794032</td>\n","      <td>13.358665</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.csv_Day7_1h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>4.880234</td>\n","      <td>13.181517</td>\n","      <td>27.436176</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.csv_Day7_1h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>5.274422</td>\n","      <td>13.634405</td>\n","      <td>22.294998</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.csv_Day7_2h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>2.656944</td>\n","      <td>12.665893</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.csv_Day7_2h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.507586</td>\n","      <td>6.913562</td>\n","      <td>0.000000</td>\n","      <td>23.632666</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.csv_Day7_3h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.csv_Day7_3h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>12.631079</td>\n","      <td>27.345919</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0.csv_Day7_4h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>6.364333</td>\n","      <td>13.665729</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0.csv_Day7_4h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>5.016449</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.csv_Day7_5h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>5.029402</td>\n","      <td>13.304340</td>\n","      <td>25.304159</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0.csv_Day7_5h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>13.662229</td>\n","      <td>29.551064</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0.csv_Day7_6h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>13.963607</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>0.csv_Day7_6h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>28.819887</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>0.csv_Day7_7h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>12.785295</td>\n","      <td>29.668720</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>0.csv_Day7_7h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>6.523306</td>\n","      <td>13.859849</td>\n","      <td>25.811575</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>0.csv_Day7_8h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>13.390540</td>\n","      <td>28.953270</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0.csv_Day7_8h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>6.461433</td>\n","      <td>12.726589</td>\n","      <td>26.067308</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>0.csv_Day7_9h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>13.770422</td>\n","      <td>25.649529</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0.csv_Day7_9h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>13.686337</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>0.csv_Day7_10h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>13.300860</td>\n","      <td>25.025518</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>0.csv_Day7_10h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>12.758828</td>\n","      <td>27.784384</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>0.csv_Day7_11h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>4.386577</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>0.csv_Day7_11h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>13.857793</td>\n","      <td>24.910563</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>0.csv_Day7_12h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>6.616251</td>\n","      <td>13.544395</td>\n","      <td>24.733349</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>0.csv_Day7_12h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.623153</td>\n","      <td>6.543428</td>\n","      <td>0.000000</td>\n","      <td>24.540684</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>0.csv_Day7_13h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>5.469880</td>\n","      <td>12.828777</td>\n","      <td>28.375969</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>0.csv_Day7_13h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>6.292616</td>\n","      <td>14.263561</td>\n","      <td>28.982252</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>0.csv_Day7_14h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>6.010474</td>\n","      <td>14.196844</td>\n","      <td>29.106680</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>0.csv_Day7_14h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>14.497191</td>\n","      <td>30.744818</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>0.csv_Day7_15h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>4.321848</td>\n","      <td>13.806555</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>0.csv_Day7_15h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>7.392370</td>\n","      <td>0.000000</td>\n","      <td>32.751968</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>0.csv_Day7_16h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>5.054944</td>\n","      <td>14.348512</td>\n","      <td>28.829498</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>0.csv_Day7_16h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>3.891443</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>0.csv_Day7_17h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>4.881485</td>\n","      <td>13.443652</td>\n","      <td>25.926998</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>0.csv_Day7_17h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>13.969741</td>\n","      <td>27.386606</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>0.csv_Day7_18h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>28.893358</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>0.csv_Day7_18h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>5.021906</td>\n","      <td>13.995449</td>\n","      <td>25.042265</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>0.csv_Day7_19h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.850775</td>\n","      <td>0.000000</td>\n","      <td>12.818525</td>\n","      <td>29.971895</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>0.csv_Day7_19h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>26.988617</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>0.csv_Day7_20h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>7.744581</td>\n","      <td>14.099709</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>0.csv_Day7_20h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>5.108526</td>\n","      <td>13.951560</td>\n","      <td>27.409472</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>0.csv_Day7_21h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.142366</td>\n","      <td>5.936326</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>0.csv_Day7_21h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.729807</td>\n","      <td>6.385531</td>\n","      <td>14.202077</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>0.csv_Day7_22h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>0.csv_Day7_22h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>4.394531</td>\n","      <td>12.922529</td>\n","      <td>26.189884</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>0.csv_Day7_23h00m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>4.448495</td>\n","      <td>14.023539</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>0.csv_Day7_23h30m</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>13.345014</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                   id  q_0.1  q_0.2  ...     q_0.7      q_0.8      q_0.9\n","0    0.csv_Day7_0h00m    0.0    0.0  ...  3.729900  13.514359  27.080038\n","1    0.csv_Day7_0h30m    0.0    0.0  ...  3.794032  13.358665   0.000000\n","2    0.csv_Day7_1h00m    0.0    0.0  ...  4.880234  13.181517  27.436176\n","3    0.csv_Day7_1h30m    0.0    0.0  ...  5.274422  13.634405  22.294998\n","4    0.csv_Day7_2h00m    0.0    0.0  ...  2.656944  12.665893   0.000000\n","5    0.csv_Day7_2h30m    0.0    0.0  ...  6.913562   0.000000  23.632666\n","6    0.csv_Day7_3h00m    0.0    0.0  ...  0.000000   0.000000   0.000000\n","7    0.csv_Day7_3h30m    0.0    0.0  ...  0.000000  12.631079  27.345919\n","8    0.csv_Day7_4h00m    0.0    0.0  ...  6.364333  13.665729   0.000000\n","9    0.csv_Day7_4h30m    0.0    0.0  ...  5.016449   0.000000   0.000000\n","10   0.csv_Day7_5h00m    0.0    0.0  ...  5.029402  13.304340  25.304159\n","11   0.csv_Day7_5h30m    0.0    0.0  ...  0.000000  13.662229  29.551064\n","12   0.csv_Day7_6h00m    0.0    0.0  ...  0.000000  13.963607   0.000000\n","13   0.csv_Day7_6h30m    0.0    0.0  ...  0.000000   0.000000  28.819887\n","14   0.csv_Day7_7h00m    0.0    0.0  ...  0.000000  12.785295  29.668720\n","15   0.csv_Day7_7h30m    0.0    0.0  ...  6.523306  13.859849  25.811575\n","16   0.csv_Day7_8h00m    0.0    0.0  ...  0.000000  13.390540  28.953270\n","17   0.csv_Day7_8h30m    0.0    0.0  ...  6.461433  12.726589  26.067308\n","18   0.csv_Day7_9h00m    0.0    0.0  ...  0.000000  13.770422  25.649529\n","19   0.csv_Day7_9h30m    0.0    0.0  ...  0.000000  13.686337   0.000000\n","20  0.csv_Day7_10h00m    0.0    0.0  ...  0.000000  13.300860  25.025518\n","21  0.csv_Day7_10h30m    0.0    0.0  ...  0.000000  12.758828  27.784384\n","22  0.csv_Day7_11h00m    0.0    0.0  ...  4.386577   0.000000   0.000000\n","23  0.csv_Day7_11h30m    0.0    0.0  ...  0.000000  13.857793  24.910563\n","24  0.csv_Day7_12h00m    0.0    0.0  ...  6.616251  13.544395  24.733349\n","25  0.csv_Day7_12h30m    0.0    0.0  ...  6.543428   0.000000  24.540684\n","26  0.csv_Day7_13h00m    0.0    0.0  ...  5.469880  12.828777  28.375969\n","27  0.csv_Day7_13h30m    0.0    0.0  ...  6.292616  14.263561  28.982252\n","28  0.csv_Day7_14h00m    0.0    0.0  ...  6.010474  14.196844  29.106680\n","29  0.csv_Day7_14h30m    0.0    0.0  ...  0.000000  14.497191  30.744818\n","30  0.csv_Day7_15h00m    0.0    0.0  ...  4.321848  13.806555   0.000000\n","31  0.csv_Day7_15h30m    0.0    0.0  ...  7.392370   0.000000  32.751968\n","32  0.csv_Day7_16h00m    0.0    0.0  ...  5.054944  14.348512  28.829498\n","33  0.csv_Day7_16h30m    0.0    0.0  ...  3.891443   0.000000   0.000000\n","34  0.csv_Day7_17h00m    0.0    0.0  ...  4.881485  13.443652  25.926998\n","35  0.csv_Day7_17h30m    0.0    0.0  ...  0.000000  13.969741  27.386606\n","36  0.csv_Day7_18h00m    0.0    0.0  ...  0.000000   0.000000  28.893358\n","37  0.csv_Day7_18h30m    0.0    0.0  ...  5.021906  13.995449  25.042265\n","38  0.csv_Day7_19h00m    0.0    0.0  ...  0.000000  12.818525  29.971895\n","39  0.csv_Day7_19h30m    0.0    0.0  ...  0.000000   0.000000  26.988617\n","40  0.csv_Day7_20h00m    0.0    0.0  ...  7.744581  14.099709   0.000000\n","41  0.csv_Day7_20h30m    0.0    0.0  ...  5.108526  13.951560  27.409472\n","42  0.csv_Day7_21h00m    0.0    0.0  ...  5.936326   0.000000   0.000000\n","43  0.csv_Day7_21h30m    0.0    0.0  ...  6.385531  14.202077   0.000000\n","44  0.csv_Day7_22h00m    0.0    0.0  ...  0.000000   0.000000   0.000000\n","45  0.csv_Day7_22h30m    0.0    0.0  ...  4.394531  12.922529  26.189884\n","46  0.csv_Day7_23h00m    0.0    0.0  ...  4.448495  14.023539   0.000000\n","47  0.csv_Day7_23h30m    0.0    0.0  ...  0.000000  13.345014   0.000000\n","\n","[48 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":385}]},{"cell_type":"code","metadata":{"id":"-g45_XP05lqy"},"source":["submission.to_csv('/content/drive/MyDrive/Dacon/Stack3.csv', index=False)"],"execution_count":null,"outputs":[]}]}