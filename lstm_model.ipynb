{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM, GRU를 이용한 시계열 발전량 예측 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "executionInfo": {
     "elapsed": 2425,
     "status": "error",
     "timestamp": 1611584827628,
     "user": {
      "displayName": "정성헌",
      "photoUrl": "",
      "userId": "12794350723891602651"
     },
     "user_tz": -540
    },
    "id": "iX5FVI_szvnZ",
    "outputId": "7add3be6-3059-49c0-836a-4962f5102daf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "pd.set_option('display.max_row', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "df_train = pd.read_csv('train/train.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "dfs_test = []\n",
    "\n",
    "for i in range(81):\n",
    "    temp_df = pd.read_csv(f\"test/{i}.csv\")\n",
    "    dfs_test.append(temp_df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gHvVGou3zvne"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "    \n",
    "def Make_New_Features(df_train):\n",
    "    df_train['GHI'] = df_train['DHI'] + df_train['DNI']\n",
    "    df_train['RDHNI'] = df_train['GHI'].apply(lambda x : x**0.5)\n",
    "#====================================================================================\n",
    "    df_train['Lower_Cloud'] = 2.5 * df_train['RH'] / 100 - 1.5\n",
    "    df_train['Middle_Cloud'] = 4 * df_train['RH'] / 100 - 3.0\n",
    "#====================================================================================\n",
    "    b = 17.62\n",
    "    c = 243.12\n",
    "    df_train['gamma'] = (b * df_train['T'] /(c + df_train['T'])) + (df_train['RH'] / 100.0).apply(lambda x: math.log(x + 1))\n",
    "    df_train['DD'] = (c * df_train['gamma']) / (b - df_train['gamma'])\n",
    "\n",
    "    df_train.drop('gamma', axis=1, inplace=True)\n",
    "#====================================================================================\n",
    "        \n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sUgv2djdzvnf"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scalers ={}\n",
    "\n",
    "def preprocess(df, make_scaler=True):\n",
    "    df = df.copy()\n",
    "    df = Make_New_Features(df)\n",
    "    \n",
    "    df['Minute'].replace(30, 0.5, inplace=True)\n",
    "    df['Time'] = df['Hour'] + df['Minute']\n",
    "    \n",
    "    df['Time_x'] = np.cos((df['Time'] * 2 * np.pi) / 24.0)\n",
    "    df['Time_y'] = np.sin((df['Time'] * 2 * np.pi) / 24.0)\n",
    "    \n",
    "    scale_target_columns = ['TARGET', 'DHI', 'DNI', 'WS', 'RH', 'T', 'TARGET', 'GHI', 'RDHNI', 'Lower_Cloud',\n",
    "                           'Middle_Cloud', 'DD']\n",
    "    \n",
    "    temp_df_agg = pd.DataFrame()\n",
    "    for c in scale_target_columns:\n",
    "        temp_df_agg[f'max_week_{c}'] = df.groupby('Day')[c].max().rolling(7, center=True).max()\n",
    "        temp_df_agg[f'min_week_{c}'] = df.groupby('Day')[c].min().rolling(7, center=True).min()\n",
    "\n",
    "        temp_df_agg[f'max_week_{c}'].iloc[:3] = temp_df_agg.iloc[3][f'max_week_{c}']\n",
    "        temp_df_agg[f'max_week_{c}'].iloc[-3:] = temp_df_agg.iloc[-4][f'max_week_{c}']\n",
    "        temp_df_agg[f'min_week_{c}'].iloc[:3] = temp_df_agg.iloc[3][f'min_week_{c}']\n",
    "        temp_df_agg[f'min_week_{c}'].iloc[-3:] = temp_df_agg.iloc[-4][f'min_week_{c}']\n",
    "        \n",
    "        \n",
    "    temp_df_agg = temp_df_agg.reset_index()\n",
    "    df = pd.merge(df, temp_df_agg, left_on='Day', right_on='Day')\n",
    "    for c in scale_target_columns:\n",
    "        df[f'scaled_{c}'] = (df[c] - df[f'min_week_{c}'] + 1) / (df[f'max_week_{c}']+1)\n",
    "\n",
    "    df['peak_diff'] = df['max_week_TARGET'] - df['TARGET']\n",
    "    df['peak_diff_scaled'] = df['peak_diff'] / df['max_week_TARGET']\n",
    "    \n",
    "    \n",
    "    normalize_target_column = ['DHI', 'DNI', 'WS', 'RH', 'T', 'GHI', 'RDHNI', 'TARGET',\n",
    "                               'Lower_Cloud', 'Middle_Cloud', 'DD']\n",
    "    \n",
    "    for c in normalize_target_column:\n",
    "        x = df[[c]].copy() #returns a numpy array\n",
    "        if make_scaler:\n",
    "            std_scaler = preprocessing.MinMaxScaler()\n",
    "            std_scaler.fit(x)\n",
    "            df[c] = std_scaler.transform(x)\n",
    "            scalers[c] = std_scaler\n",
    "        else:\n",
    "            df[c] = scalers[c].transform(x)\n",
    "    \n",
    "    df = df.drop(['Hour', 'Minute', 'Time'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = preprocess(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bQ0Uj4C7zvnh"
   },
   "outputs": [],
   "source": [
    "temp = []\n",
    "for df_test in dfs_test:\n",
    "    temp.append(preprocess(df_test, make_scaler=False))\n",
    "dfs = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GiPsUhEVzvni",
    "outputId": "2bd7de73-0075-4b2e-9ece-8182f0dd3b59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52128, 336, 15), (52128, 48, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_train_data_set(df):\n",
    "    df = df.astype('float')\n",
    "    ins_datas = df[['DHI', 'DNI', 'WS', 'RH', 'T', 'TARGET', 'GHI', 'RDHNI', 'Lower_Cloud',\n",
    "                   'Middle_Cloud', 'DD', 'Time_x', 'Time_y',\n",
    "                   'scaled_TARGET', 'peak_diff_scaled']].values\n",
    "    outs_datas = df[['TARGET']].values\n",
    "    \n",
    "    ins = []\n",
    "    outs = []\n",
    "    \n",
    "    for i in range(len(df)-48*9):\n",
    "        ins.append(ins_datas[i:i+48*7])\n",
    "        outs.append(outs_datas[i+48*7:i+48*8])\n",
    "        \n",
    "    return np.array(ins), np.array(outs)\n",
    "\n",
    "def make_prediction_data_set(df):\n",
    "    df = df.astype('float')\n",
    "    ins_datas = df[['DHI', 'DNI', 'WS', 'RH', 'T', 'TARGET', 'GHI', 'RDHNI', 'Lower_Cloud',\n",
    "                   'Middle_Cloud', 'DD', 'Time_x', 'Time_y',\n",
    "                   'scaled_TARGET', 'peak_diff_scaled']].values\n",
    "    \n",
    "    ins = [ins_datas]\n",
    "\n",
    "    return np.array(ins)\n",
    "\n",
    "ins, outs = make_train_data_set(df)\n",
    "ins.shape, outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "U4wuqg9Tzvnm",
    "outputId": "e4e73588-11cd-4fb2-f5e6-b292fbb3e008"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17376, 336, 15),\n",
       " (17376, 336, 15),\n",
       " (17376, 336, 15),\n",
       " (17376, 48, 1),\n",
       " (17376, 48, 1),\n",
       " (17376, 48, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term = 17376\n",
    "\n",
    "x_year_1 = ins[:term]\n",
    "x_year_2 = ins[term:term*2]\n",
    "x_year_3 = ins[term*2:]\n",
    "\n",
    "y_year_1 = outs[:term]\n",
    "y_year_2 = outs[term:term*2]\n",
    "y_year_3 = outs[term*2:]\n",
    "\n",
    "drop_columns=[]\n",
    "x_year_1.shape, x_year_2.shape, x_year_3.shape, y_year_1.shape, y_year_2.shape, y_year_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0YQXWuWhzvnm",
    "outputId": "0aa46716-d08b-4b9b-de4d-9eb4f8ec7f5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 336, 15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "for df in dfs:\n",
    "    temp.append(make_prediction_data_set(df))\n",
    "pre_ins = np.concatenate(temp)\n",
    "pre_ins.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cw76X5Oizvnn"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU, Conv1D\n",
    "from keras.layers import Flatten, Reshape\n",
    "from keras.layers import LSTM, Input, Lambda, Concatenate\n",
    "from keras.layers import RepeatVector, MaxPooling1D\n",
    "from keras.layers import TimeDistributed, Bidirectional\n",
    "from keras import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "# 7일의 데이터로 2일을 예측\n",
    "# 1일 = 48개의 관측값\n",
    "# 7일 = 7 * 48 = 336\n",
    "# 2일 = 2 * 48 = 96\n",
    "def pinball_loss(tau):\n",
    "    def loss(y_true, y_pred):\n",
    "        err = y_true - y_pred\n",
    "        return K.mean(K.maximum(tau * err, (tau - 1) * err))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def build_model(tau, lr=0.001, use_summary=True, use_mse=False):\n",
    "    opt = optimizers.Adam(lr=lr)\n",
    "    \n",
    "    x_lstm = Input(shape=(336, 15))\n",
    "    \n",
    "    x_ins = []\n",
    "    for i in range(7):\n",
    "        x_ins.append(Lambda(lambda x: x[:, 48*i:48*(i+1), :])(x_lstm))\n",
    "    \n",
    "    x_s = []\n",
    "    for x in x_ins:\n",
    "        x = GRU(100, return_sequences=True)(x)\n",
    "        x = GRU(80, return_sequences=False)(x)\n",
    "        x_s.append(x)\n",
    "    \n",
    "    x = Concatenate()(x_s)\n",
    "    x = Reshape(target_shape=(7, 80))(x)\n",
    "    x = Bidirectional(LSTM(120, return_sequences=False))(x)\n",
    "    x = Dense(48, activation='relu')(x)\n",
    "\n",
    "    model = Model(x_lstm, x)\n",
    "    \n",
    "    if use_mse:\n",
    "        model.compile(loss='mse', optimizer=opt)\n",
    "    else:\n",
    "        model.compile(loss=pinball_loss(tau), optimizer=opt)\n",
    "        \n",
    "    if use_summary:\n",
    "        model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rXig8RbJzvnv"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def do_dl(q, verbose=0):\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True,)\n",
    "    \n",
    "    model_1 = build_model(q, 0.0001, use_summary=False)\n",
    "    his = model_1.fit(np.concatenate([x_year_1, x_year_2]),\n",
    "                     np.concatenate([y_year_1, y_year_2]),\n",
    "                     epochs=300, batch_size=2000, validation_data=(x_year_3, y_year_3),\n",
    "                     verbose=verbose, callbacks=[es])\n",
    "    model_1.save(f'models/ensemble/model_1_lstm_1_8_{q}_1_fined')\n",
    "\n",
    "    model_2 = build_model(q, 0.0001, use_summary=False)\n",
    "    his = model_2.fit(np.concatenate([x_year_1, x_year_3]),\n",
    "                     np.concatenate([y_year_1, y_year_3]),\n",
    "                     epochs=300, batch_size=2000, validation_data=(x_year_2, y_year_2),\n",
    "                     verbose=verbose, callbacks=[es])\n",
    "    model_2.save(f'models/ensemble/model_2_lstm_1_8_{q}_1_fined')\n",
    "\n",
    "    model_3 = build_model(q, 0.0001, use_summary=False)\n",
    "    his = model_3.fit(np.concatenate([x_year_2, x_year_3]),\n",
    "                     np.concatenate([y_year_2, y_year_3]),\n",
    "                     epochs=300, batch_size=2000, validation_data=(x_year_1, y_year_1),\n",
    "                     verbose=verbose, callbacks=[es])\n",
    "    model_3.save(f'models/ensemble/model_3_lstm_1_8_{q}_1_fined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6h4ZGBPzvnw",
    "outputId": "ce5ba5ec-2f89-405f-e19e-e418e3ff5794",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/ensemble/model_1_lstm_1_8_0.2_1_fined\\assets\n",
      "INFO:tensorflow:Assets written to: models/ensemble/model_2_lstm_1_8_0.2_1_fined\\assets\n",
      "INFO:tensorflow:Assets written to: models/ensemble/model_3_lstm_1_8_0.2_1_fined\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████                                                                      | 1/8 [24:22<2:50:40, 1462.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/ensemble/model_1_lstm_1_8_0.3_1_fined\\assets\n",
      "INFO:tensorflow:Assets written to: models/ensemble/model_2_lstm_1_8_0.3_1_fined\\assets\n",
      "INFO:tensorflow:Assets written to: models/ensemble/model_3_lstm_1_8_0.3_1_fined\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████████                                                            | 2/8 [58:49<2:44:24, 1644.01s/it]"
     ]
    }
   ],
   "source": [
    "for q in tqdm(taus[1:]):\n",
    "    do_dl(q, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2X0HAZVzvn1"
   },
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "\n",
    "columns = ['q_0.1', 'q_0.2', 'q_0.3', 'q_0.4', 'q_0.5', 'q_0.6', 'q_0.7', 'q_0.8', 'q_0.9']\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    out = do_predict(models[i])\n",
    "    df_sample[columns[i]] = out\n",
    "    \n",
    "df_sample.to_csv(f'result_branch_from_main_model_test_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKZZM1Hmzvn1"
   },
   "outputs": [],
   "source": [
    "for h in hiss:\n",
    "    plt.plot(h.history['loss'])\n",
    "    plt.plot(h.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "lstm_model_1_8.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
