{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52560 entries, 0 to 52559\n",
      "Data columns (total 9 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Day     52560 non-null  int64  \n",
      " 1   Hour    52560 non-null  int64  \n",
      " 2   Minute  52560 non-null  int64  \n",
      " 3   DHI     52560 non-null  int64  \n",
      " 4   DNI     52560 non-null  int64  \n",
      " 5   WS      52560 non-null  float64\n",
      " 6   RH      52560 non-null  float64\n",
      " 7   T       52560 non-null  int64  \n",
      " 8   TARGET  52560 non-null  float64\n",
      "dtypes: float64(3), int64(6)\n",
      "memory usage: 3.6 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pandas_profiling\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "pd.set_option('display.max_row', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "df = pd.read_csv('train/train.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Minute'].replace(30, 0.5, inplace=True)\n",
    "df['Time'] = df['Hour'] + df['Minute']\n",
    "df = df.drop(['Hour', 'Minute'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하루 48 -> 29\n",
    "df_none_zero = df[(df['Time'] >= 5.0) & (df['Time'] <= 19.0)]\n",
    "df_none_zero.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qkrwl\\anaconda3\\envs\\kaggle\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\qkrwl\\anaconda3\\envs\\kaggle\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\qkrwl\\anaconda3\\envs\\kaggle\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\qkrwl\\anaconda3\\envs\\kaggle\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\qkrwl\\anaconda3\\envs\\kaggle\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\qkrwl\\anaconda3\\envs\\kaggle\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\qkrwl\\anaconda3\\envs\\kaggle\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\qkrwl\\anaconda3\\envs\\kaggle\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHI_mean</th>\n",
       "      <th>DHI_max</th>\n",
       "      <th>DHI_std</th>\n",
       "      <th>DHI_var</th>\n",
       "      <th>DHI_sum</th>\n",
       "      <th>DHI_median</th>\n",
       "      <th>DNI_mean</th>\n",
       "      <th>DNI_max</th>\n",
       "      <th>DNI_std</th>\n",
       "      <th>DNI_var</th>\n",
       "      <th>DNI_sum</th>\n",
       "      <th>DNI_median</th>\n",
       "      <th>WS_mean</th>\n",
       "      <th>WS_min</th>\n",
       "      <th>WS_max</th>\n",
       "      <th>WS_std</th>\n",
       "      <th>WS_var</th>\n",
       "      <th>WS_sum</th>\n",
       "      <th>WS_median</th>\n",
       "      <th>RH_mean</th>\n",
       "      <th>RH_min</th>\n",
       "      <th>RH_max</th>\n",
       "      <th>RH_std</th>\n",
       "      <th>RH_var</th>\n",
       "      <th>RH_sum</th>\n",
       "      <th>RH_median</th>\n",
       "      <th>T_mean</th>\n",
       "      <th>T_min</th>\n",
       "      <th>T_max</th>\n",
       "      <th>T_std</th>\n",
       "      <th>T_var</th>\n",
       "      <th>T_sum</th>\n",
       "      <th>T_median</th>\n",
       "      <th>Day_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.140870</td>\n",
       "      <td>0.403409</td>\n",
       "      <td>0.152041</td>\n",
       "      <td>0.023117</td>\n",
       "      <td>4.085227</td>\n",
       "      <td>0.109848</td>\n",
       "      <td>0.123018</td>\n",
       "      <td>0.765817</td>\n",
       "      <td>0.210889</td>\n",
       "      <td>0.044474</td>\n",
       "      <td>3.567517</td>\n",
       "      <td>0.006610</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.044208</td>\n",
       "      <td>0.001954</td>\n",
       "      <td>5.075000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.629918</td>\n",
       "      <td>0.493020</td>\n",
       "      <td>0.748079</td>\n",
       "      <td>0.077597</td>\n",
       "      <td>0.006021</td>\n",
       "      <td>18.267612</td>\n",
       "      <td>0.626123</td>\n",
       "      <td>0.231801</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.057211</td>\n",
       "      <td>0.003273</td>\n",
       "      <td>6.722222</td>\n",
       "      <td>0.240741</td>\n",
       "      <td>0.62069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.058320</td>\n",
       "      <td>0.126894</td>\n",
       "      <td>0.052898</td>\n",
       "      <td>0.002798</td>\n",
       "      <td>1.691288</td>\n",
       "      <td>0.053030</td>\n",
       "      <td>0.461366</td>\n",
       "      <td>0.925401</td>\n",
       "      <td>0.414326</td>\n",
       "      <td>0.171666</td>\n",
       "      <td>13.379603</td>\n",
       "      <td>0.605288</td>\n",
       "      <td>0.133908</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.065197</td>\n",
       "      <td>0.004251</td>\n",
       "      <td>3.883333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.663512</td>\n",
       "      <td>0.499838</td>\n",
       "      <td>0.863002</td>\n",
       "      <td>0.111521</td>\n",
       "      <td>0.012437</td>\n",
       "      <td>19.241857</td>\n",
       "      <td>0.663565</td>\n",
       "      <td>0.275223</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.088912</td>\n",
       "      <td>0.007905</td>\n",
       "      <td>7.981481</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.62069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.090713</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.101894</td>\n",
       "      <td>0.010382</td>\n",
       "      <td>2.630682</td>\n",
       "      <td>0.053030</td>\n",
       "      <td>0.038520</td>\n",
       "      <td>0.505194</td>\n",
       "      <td>0.116973</td>\n",
       "      <td>0.013683</td>\n",
       "      <td>1.117092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.256609</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.441667</td>\n",
       "      <td>0.096886</td>\n",
       "      <td>0.009387</td>\n",
       "      <td>7.441667</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.745624</td>\n",
       "      <td>0.605454</td>\n",
       "      <td>0.860946</td>\n",
       "      <td>0.074585</td>\n",
       "      <td>0.005563</td>\n",
       "      <td>21.623093</td>\n",
       "      <td>0.740180</td>\n",
       "      <td>0.272031</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.066788</td>\n",
       "      <td>0.004461</td>\n",
       "      <td>7.888889</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.62069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.123237</td>\n",
       "      <td>0.399621</td>\n",
       "      <td>0.134937</td>\n",
       "      <td>0.018208</td>\n",
       "      <td>3.573864</td>\n",
       "      <td>0.064394</td>\n",
       "      <td>0.104686</td>\n",
       "      <td>0.860246</td>\n",
       "      <td>0.189817</td>\n",
       "      <td>0.036031</td>\n",
       "      <td>3.035883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.049950</td>\n",
       "      <td>0.002495</td>\n",
       "      <td>6.041667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.583916</td>\n",
       "      <td>0.448977</td>\n",
       "      <td>0.703820</td>\n",
       "      <td>0.068998</td>\n",
       "      <td>0.004761</td>\n",
       "      <td>16.933557</td>\n",
       "      <td>0.614327</td>\n",
       "      <td>0.284802</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.079252</td>\n",
       "      <td>0.006281</td>\n",
       "      <td>8.259259</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.62069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.138323</td>\n",
       "      <td>0.412879</td>\n",
       "      <td>0.147664</td>\n",
       "      <td>0.021805</td>\n",
       "      <td>4.011364</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.035167</td>\n",
       "      <td>0.207743</td>\n",
       "      <td>0.062211</td>\n",
       "      <td>0.003870</td>\n",
       "      <td>1.019830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450862</td>\n",
       "      <td>0.191667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.152449</td>\n",
       "      <td>0.023241</td>\n",
       "      <td>13.075000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.695883</td>\n",
       "      <td>0.615951</td>\n",
       "      <td>0.807813</td>\n",
       "      <td>0.058321</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>20.180608</td>\n",
       "      <td>0.695163</td>\n",
       "      <td>0.398467</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.462963</td>\n",
       "      <td>0.055251</td>\n",
       "      <td>0.003053</td>\n",
       "      <td>11.555556</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.62069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.113310</td>\n",
       "      <td>0.297348</td>\n",
       "      <td>0.104975</td>\n",
       "      <td>0.011020</td>\n",
       "      <td>3.285985</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.253101</td>\n",
       "      <td>0.702550</td>\n",
       "      <td>0.258811</td>\n",
       "      <td>0.066983</td>\n",
       "      <td>7.339943</td>\n",
       "      <td>0.221907</td>\n",
       "      <td>0.229598</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.040495</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>6.658333</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.755553</td>\n",
       "      <td>0.570826</td>\n",
       "      <td>0.956931</td>\n",
       "      <td>0.113119</td>\n",
       "      <td>0.012796</td>\n",
       "      <td>21.911049</td>\n",
       "      <td>0.777946</td>\n",
       "      <td>0.428480</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.064857</td>\n",
       "      <td>0.004206</td>\n",
       "      <td>12.425926</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.62069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.115596</td>\n",
       "      <td>0.412879</td>\n",
       "      <td>0.142529</td>\n",
       "      <td>0.020315</td>\n",
       "      <td>3.352273</td>\n",
       "      <td>0.070076</td>\n",
       "      <td>0.125330</td>\n",
       "      <td>0.823418</td>\n",
       "      <td>0.254324</td>\n",
       "      <td>0.064681</td>\n",
       "      <td>3.634561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107471</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.043777</td>\n",
       "      <td>0.001916</td>\n",
       "      <td>3.116667</td>\n",
       "      <td>0.091667</td>\n",
       "      <td>0.693573</td>\n",
       "      <td>0.539552</td>\n",
       "      <td>0.855319</td>\n",
       "      <td>0.103570</td>\n",
       "      <td>0.010727</td>\n",
       "      <td>20.113624</td>\n",
       "      <td>0.694730</td>\n",
       "      <td>0.328863</td>\n",
       "      <td>0.240741</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.053226</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>9.537037</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.62069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.061259</td>\n",
       "      <td>0.128788</td>\n",
       "      <td>0.053853</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.776515</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.488262</td>\n",
       "      <td>0.915014</td>\n",
       "      <td>0.407502</td>\n",
       "      <td>0.166058</td>\n",
       "      <td>14.159585</td>\n",
       "      <td>0.685552</td>\n",
       "      <td>0.226724</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.092297</td>\n",
       "      <td>0.008519</td>\n",
       "      <td>6.575000</td>\n",
       "      <td>0.191667</td>\n",
       "      <td>0.676069</td>\n",
       "      <td>0.476031</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.169303</td>\n",
       "      <td>0.028664</td>\n",
       "      <td>19.605995</td>\n",
       "      <td>0.608268</td>\n",
       "      <td>0.415709</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.065582</td>\n",
       "      <td>0.004301</td>\n",
       "      <td>12.055556</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.62069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.135906</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.154905</td>\n",
       "      <td>0.023996</td>\n",
       "      <td>3.941288</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.244733</td>\n",
       "      <td>0.861190</td>\n",
       "      <td>0.306095</td>\n",
       "      <td>0.093694</td>\n",
       "      <td>7.097262</td>\n",
       "      <td>0.083097</td>\n",
       "      <td>0.112644</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.030344</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>3.266667</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.684106</td>\n",
       "      <td>0.541067</td>\n",
       "      <td>0.847419</td>\n",
       "      <td>0.105450</td>\n",
       "      <td>0.011120</td>\n",
       "      <td>19.839087</td>\n",
       "      <td>0.715832</td>\n",
       "      <td>0.337803</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.425926</td>\n",
       "      <td>0.067437</td>\n",
       "      <td>0.004548</td>\n",
       "      <td>9.796296</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.62069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.085031</td>\n",
       "      <td>0.202652</td>\n",
       "      <td>0.076803</td>\n",
       "      <td>0.005899</td>\n",
       "      <td>2.465909</td>\n",
       "      <td>0.094697</td>\n",
       "      <td>0.389502</td>\n",
       "      <td>0.883853</td>\n",
       "      <td>0.354802</td>\n",
       "      <td>0.125885</td>\n",
       "      <td>11.295562</td>\n",
       "      <td>0.331445</td>\n",
       "      <td>0.191379</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.100136</td>\n",
       "      <td>0.010027</td>\n",
       "      <td>5.550000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.653758</td>\n",
       "      <td>0.457202</td>\n",
       "      <td>0.830971</td>\n",
       "      <td>0.118677</td>\n",
       "      <td>0.014084</td>\n",
       "      <td>18.958987</td>\n",
       "      <td>0.671789</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.065711</td>\n",
       "      <td>0.004318</td>\n",
       "      <td>12.166667</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.62069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DHI_mean   DHI_max   DHI_std   DHI_var   DHI_sum  DHI_median  DNI_mean  \\\n",
       "0  0.140870  0.403409  0.152041  0.023117  4.085227    0.109848  0.123018   \n",
       "1  0.058320  0.126894  0.052898  0.002798  1.691288    0.053030  0.461366   \n",
       "2  0.090713  0.333333  0.101894  0.010382  2.630682    0.053030  0.038520   \n",
       "3  0.123237  0.399621  0.134937  0.018208  3.573864    0.064394  0.104686   \n",
       "4  0.138323  0.412879  0.147664  0.021805  4.011364    0.083333  0.035167   \n",
       "5  0.113310  0.297348  0.104975  0.011020  3.285985    0.104167  0.253101   \n",
       "6  0.115596  0.412879  0.142529  0.020315  3.352273    0.070076  0.125330   \n",
       "7  0.061259  0.128788  0.053853  0.002900  1.776515    0.068182  0.488262   \n",
       "8  0.135906  0.424242  0.154905  0.023996  3.941288    0.083333  0.244733   \n",
       "9  0.085031  0.202652  0.076803  0.005899  2.465909    0.094697  0.389502   \n",
       "\n",
       "    DNI_max   DNI_std   DNI_var    DNI_sum  DNI_median   WS_mean    WS_min  \\\n",
       "0  0.765817  0.210889  0.044474   3.567517    0.006610  0.175000  0.108333   \n",
       "1  0.925401  0.414326  0.171666  13.379603    0.605288  0.133908  0.033333   \n",
       "2  0.505194  0.116973  0.013683   1.117092    0.000000  0.256609  0.125000   \n",
       "3  0.860246  0.189817  0.036031   3.035883    0.000000  0.208333  0.125000   \n",
       "4  0.207743  0.062211  0.003870   1.019830    0.000000  0.450862  0.191667   \n",
       "5  0.702550  0.258811  0.066983   7.339943    0.221907  0.229598  0.158333   \n",
       "6  0.823418  0.254324  0.064681   3.634561    0.000000  0.107471  0.075000   \n",
       "7  0.915014  0.407502  0.166058  14.159585    0.685552  0.226724  0.108333   \n",
       "8  0.861190  0.306095  0.093694   7.097262    0.083097  0.112644  0.041667   \n",
       "9  0.883853  0.354802  0.125885  11.295562    0.331445  0.191379  0.025000   \n",
       "\n",
       "     WS_max    WS_std    WS_var     WS_sum  WS_median   RH_mean    RH_min  \\\n",
       "0  0.266667  0.044208  0.001954   5.075000   0.166667  0.629918  0.493020   \n",
       "1  0.275000  0.065197  0.004251   3.883333   0.133333  0.663512  0.499838   \n",
       "2  0.441667  0.096886  0.009387   7.441667   0.216667  0.745624  0.605454   \n",
       "3  0.300000  0.049950  0.002495   6.041667   0.200000  0.583916  0.448977   \n",
       "4  0.666667  0.152449  0.023241  13.075000   0.450000  0.695883  0.615951   \n",
       "5  0.283333  0.040495  0.001640   6.658333   0.225000  0.755553  0.570826   \n",
       "6  0.250000  0.043777  0.001916   3.116667   0.091667  0.693573  0.539552   \n",
       "7  0.383333  0.092297  0.008519   6.575000   0.191667  0.676069  0.476031   \n",
       "8  0.175000  0.030344  0.000921   3.266667   0.116667  0.684106  0.541067   \n",
       "9  0.316667  0.100136  0.010027   5.550000   0.250000  0.653758  0.457202   \n",
       "\n",
       "     RH_max    RH_std    RH_var     RH_sum  RH_median    T_mean     T_min  \\\n",
       "0  0.748079  0.077597  0.006021  18.267612   0.626123  0.231801  0.129630   \n",
       "1  0.863002  0.111521  0.012437  19.241857   0.663565  0.275223  0.129630   \n",
       "2  0.860946  0.074585  0.005563  21.623093   0.740180  0.272031  0.148148   \n",
       "3  0.703820  0.068998  0.004761  16.933557   0.614327  0.284802  0.129630   \n",
       "4  0.807813  0.058321  0.003401  20.180608   0.695163  0.398467  0.277778   \n",
       "5  0.956931  0.113119  0.012796  21.911049   0.777946  0.428480  0.351852   \n",
       "6  0.855319  0.103570  0.010727  20.113624   0.694730  0.328863  0.240741   \n",
       "7  1.000000  0.169303  0.028664  19.605995   0.608268  0.415709  0.314815   \n",
       "8  0.847419  0.105450  0.011120  19.839087   0.715832  0.337803  0.222222   \n",
       "9  0.830971  0.118677  0.014084  18.958987   0.671789  0.419540  0.333333   \n",
       "\n",
       "      T_max     T_std     T_var      T_sum  T_median  Day_length  \n",
       "0  0.296296  0.057211  0.003273   6.722222  0.240741     0.62069  \n",
       "1  0.388889  0.088912  0.007905   7.981481  0.296296     0.62069  \n",
       "2  0.351852  0.066788  0.004461   7.888889  0.277778     0.62069  \n",
       "3  0.370370  0.079252  0.006281   8.259259  0.314815     0.62069  \n",
       "4  0.462963  0.055251  0.003053  11.555556  0.407407     0.62069  \n",
       "5  0.518519  0.064857  0.004206  12.425926  0.407407     0.62069  \n",
       "6  0.407407  0.053226  0.002833   9.537037  0.333333     0.62069  \n",
       "7  0.500000  0.065582  0.004301  12.055556  0.407407     0.62069  \n",
       "8  0.425926  0.067437  0.004548   9.796296  0.351852     0.62069  \n",
       "9  0.518519  0.065711  0.004318  12.166667  0.407407     0.62069  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_none_zero['Origin_TARGET'] = df_none_zero['TARGET'].copy()\n",
    "\n",
    "normalize_target_column = ['DHI', 'DNI', 'WS','RH','T', 'Time', 'TARGET']\n",
    "\n",
    "for c in normalize_target_column:\n",
    "    x = df_none_zero[[c]].copy() #returns a numpy array\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    df_none_zero[c] = x_scaled\n",
    "\n",
    "df_agg = df_none_zero.groupby('Day').agg({\n",
    "    # DHI, DNI의 min 값은 모두 0, 매일 29개의 row 중 빛이 없는 경우는 반드시 있으므로\n",
    "    # 따라서 DHI, DNI의 min은 제외\n",
    "    'DHI': ['mean','max','std','var','sum','median'],\n",
    "    'DNI': ['mean','max','std','var','sum','median'],\n",
    "    'WS': ['mean','min','max','std','var','sum','median'],\n",
    "    'RH': ['mean','min','max','std','var','sum','median'],\n",
    "    'T': ['mean','min','max','std','var','sum','median'],\n",
    "})\n",
    "\n",
    "df_agg.columns = [('_').join(column) for column in df_agg.columns.ravel()]\n",
    "df_agg = df_agg.reset_index(drop=True)\n",
    "df_agg['Day_length'] = df_none_zero[df_none_zero['TARGET'] != 0.0].groupby('Day')['Time'].count()\n",
    "df_agg['Day_length'] /= df_agg['Day_length'].max()\n",
    "df_agg.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHI_mean</th>\n",
       "      <th>DHI_max</th>\n",
       "      <th>DHI_std</th>\n",
       "      <th>DHI_var</th>\n",
       "      <th>DHI_sum</th>\n",
       "      <th>DHI_median</th>\n",
       "      <th>DNI_mean</th>\n",
       "      <th>DNI_max</th>\n",
       "      <th>DNI_std</th>\n",
       "      <th>DNI_var</th>\n",
       "      <th>DNI_sum</th>\n",
       "      <th>DNI_median</th>\n",
       "      <th>WS_mean</th>\n",
       "      <th>WS_min</th>\n",
       "      <th>WS_max</th>\n",
       "      <th>WS_std</th>\n",
       "      <th>WS_var</th>\n",
       "      <th>WS_sum</th>\n",
       "      <th>WS_median</th>\n",
       "      <th>RH_mean</th>\n",
       "      <th>RH_min</th>\n",
       "      <th>RH_max</th>\n",
       "      <th>RH_std</th>\n",
       "      <th>RH_var</th>\n",
       "      <th>RH_sum</th>\n",
       "      <th>RH_median</th>\n",
       "      <th>T_mean</th>\n",
       "      <th>T_min</th>\n",
       "      <th>T_max</th>\n",
       "      <th>T_std</th>\n",
       "      <th>T_var</th>\n",
       "      <th>T_sum</th>\n",
       "      <th>T_median</th>\n",
       "      <th>Day_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.140870</td>\n",
       "      <td>0.403409</td>\n",
       "      <td>0.152041</td>\n",
       "      <td>0.023117</td>\n",
       "      <td>4.085227</td>\n",
       "      <td>0.109848</td>\n",
       "      <td>0.123018</td>\n",
       "      <td>0.765817</td>\n",
       "      <td>0.210889</td>\n",
       "      <td>0.044474</td>\n",
       "      <td>3.567517</td>\n",
       "      <td>0.006610</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.044208</td>\n",
       "      <td>0.001954</td>\n",
       "      <td>5.075000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.629918</td>\n",
       "      <td>0.493020</td>\n",
       "      <td>0.748079</td>\n",
       "      <td>0.077597</td>\n",
       "      <td>0.006021</td>\n",
       "      <td>18.267612</td>\n",
       "      <td>0.626123</td>\n",
       "      <td>0.231801</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.057211</td>\n",
       "      <td>0.003273</td>\n",
       "      <td>6.722222</td>\n",
       "      <td>0.240741</td>\n",
       "      <td>0.62069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.058320</td>\n",
       "      <td>0.126894</td>\n",
       "      <td>0.052898</td>\n",
       "      <td>0.002798</td>\n",
       "      <td>1.691288</td>\n",
       "      <td>0.053030</td>\n",
       "      <td>0.461366</td>\n",
       "      <td>0.925401</td>\n",
       "      <td>0.414326</td>\n",
       "      <td>0.171666</td>\n",
       "      <td>13.379603</td>\n",
       "      <td>0.605288</td>\n",
       "      <td>0.133908</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.065197</td>\n",
       "      <td>0.004251</td>\n",
       "      <td>3.883333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.663512</td>\n",
       "      <td>0.499838</td>\n",
       "      <td>0.863002</td>\n",
       "      <td>0.111521</td>\n",
       "      <td>0.012437</td>\n",
       "      <td>19.241857</td>\n",
       "      <td>0.663565</td>\n",
       "      <td>0.275223</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.088912</td>\n",
       "      <td>0.007905</td>\n",
       "      <td>7.981481</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.62069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.090713</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.101894</td>\n",
       "      <td>0.010382</td>\n",
       "      <td>2.630682</td>\n",
       "      <td>0.053030</td>\n",
       "      <td>0.038520</td>\n",
       "      <td>0.505194</td>\n",
       "      <td>0.116973</td>\n",
       "      <td>0.013683</td>\n",
       "      <td>1.117092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.256609</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.441667</td>\n",
       "      <td>0.096886</td>\n",
       "      <td>0.009387</td>\n",
       "      <td>7.441667</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.745624</td>\n",
       "      <td>0.605454</td>\n",
       "      <td>0.860946</td>\n",
       "      <td>0.074585</td>\n",
       "      <td>0.005563</td>\n",
       "      <td>21.623093</td>\n",
       "      <td>0.740180</td>\n",
       "      <td>0.272031</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.066788</td>\n",
       "      <td>0.004461</td>\n",
       "      <td>7.888889</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.62069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.123237</td>\n",
       "      <td>0.399621</td>\n",
       "      <td>0.134937</td>\n",
       "      <td>0.018208</td>\n",
       "      <td>3.573864</td>\n",
       "      <td>0.064394</td>\n",
       "      <td>0.104686</td>\n",
       "      <td>0.860246</td>\n",
       "      <td>0.189817</td>\n",
       "      <td>0.036031</td>\n",
       "      <td>3.035883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.049950</td>\n",
       "      <td>0.002495</td>\n",
       "      <td>6.041667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.583916</td>\n",
       "      <td>0.448977</td>\n",
       "      <td>0.703820</td>\n",
       "      <td>0.068998</td>\n",
       "      <td>0.004761</td>\n",
       "      <td>16.933557</td>\n",
       "      <td>0.614327</td>\n",
       "      <td>0.284802</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.079252</td>\n",
       "      <td>0.006281</td>\n",
       "      <td>8.259259</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.62069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.138323</td>\n",
       "      <td>0.412879</td>\n",
       "      <td>0.147664</td>\n",
       "      <td>0.021805</td>\n",
       "      <td>4.011364</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.035167</td>\n",
       "      <td>0.207743</td>\n",
       "      <td>0.062211</td>\n",
       "      <td>0.003870</td>\n",
       "      <td>1.019830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450862</td>\n",
       "      <td>0.191667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.152449</td>\n",
       "      <td>0.023241</td>\n",
       "      <td>13.075000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.695883</td>\n",
       "      <td>0.615951</td>\n",
       "      <td>0.807813</td>\n",
       "      <td>0.058321</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>20.180608</td>\n",
       "      <td>0.695163</td>\n",
       "      <td>0.398467</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.462963</td>\n",
       "      <td>0.055251</td>\n",
       "      <td>0.003053</td>\n",
       "      <td>11.555556</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.62069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>0.058712</td>\n",
       "      <td>0.130682</td>\n",
       "      <td>0.053209</td>\n",
       "      <td>0.002831</td>\n",
       "      <td>1.702652</td>\n",
       "      <td>0.064394</td>\n",
       "      <td>0.478949</td>\n",
       "      <td>0.923513</td>\n",
       "      <td>0.406235</td>\n",
       "      <td>0.165027</td>\n",
       "      <td>13.889518</td>\n",
       "      <td>0.654391</td>\n",
       "      <td>0.270115</td>\n",
       "      <td>0.141667</td>\n",
       "      <td>0.391667</td>\n",
       "      <td>0.070271</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>7.833333</td>\n",
       "      <td>0.258333</td>\n",
       "      <td>0.502767</td>\n",
       "      <td>0.372687</td>\n",
       "      <td>0.595282</td>\n",
       "      <td>0.073293</td>\n",
       "      <td>0.005372</td>\n",
       "      <td>14.580240</td>\n",
       "      <td>0.536847</td>\n",
       "      <td>0.371648</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.462963</td>\n",
       "      <td>0.063175</td>\n",
       "      <td>0.003991</td>\n",
       "      <td>10.777778</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.62069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>0.093521</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>0.089891</td>\n",
       "      <td>0.008080</td>\n",
       "      <td>2.712121</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.322881</td>\n",
       "      <td>0.744098</td>\n",
       "      <td>0.287719</td>\n",
       "      <td>0.082782</td>\n",
       "      <td>9.363551</td>\n",
       "      <td>0.435316</td>\n",
       "      <td>0.122414</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.042789</td>\n",
       "      <td>0.001831</td>\n",
       "      <td>3.550000</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.512301</td>\n",
       "      <td>0.357862</td>\n",
       "      <td>0.717022</td>\n",
       "      <td>0.115103</td>\n",
       "      <td>0.013249</td>\n",
       "      <td>14.856725</td>\n",
       "      <td>0.484471</td>\n",
       "      <td>0.385696</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.052860</td>\n",
       "      <td>0.002794</td>\n",
       "      <td>11.185185</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.62069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>0.108346</td>\n",
       "      <td>0.350379</td>\n",
       "      <td>0.108488</td>\n",
       "      <td>0.011770</td>\n",
       "      <td>3.142045</td>\n",
       "      <td>0.102273</td>\n",
       "      <td>0.267722</td>\n",
       "      <td>0.812087</td>\n",
       "      <td>0.263637</td>\n",
       "      <td>0.069505</td>\n",
       "      <td>7.763928</td>\n",
       "      <td>0.307838</td>\n",
       "      <td>0.276724</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.139965</td>\n",
       "      <td>0.019590</td>\n",
       "      <td>8.025000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.428275</td>\n",
       "      <td>0.353533</td>\n",
       "      <td>0.546911</td>\n",
       "      <td>0.054368</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>12.419976</td>\n",
       "      <td>0.415215</td>\n",
       "      <td>0.425926</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.063382</td>\n",
       "      <td>0.004017</td>\n",
       "      <td>12.351852</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.62069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>0.052377</td>\n",
       "      <td>0.164773</td>\n",
       "      <td>0.047962</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.518939</td>\n",
       "      <td>0.064394</td>\n",
       "      <td>0.484224</td>\n",
       "      <td>0.933900</td>\n",
       "      <td>0.405847</td>\n",
       "      <td>0.164711</td>\n",
       "      <td>14.042493</td>\n",
       "      <td>0.693107</td>\n",
       "      <td>0.302299</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.408333</td>\n",
       "      <td>0.080975</td>\n",
       "      <td>0.006557</td>\n",
       "      <td>8.766667</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.373030</td>\n",
       "      <td>0.229737</td>\n",
       "      <td>0.506439</td>\n",
       "      <td>0.072695</td>\n",
       "      <td>0.005285</td>\n",
       "      <td>10.817877</td>\n",
       "      <td>0.388161</td>\n",
       "      <td>0.427842</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.537037</td>\n",
       "      <td>0.070750</td>\n",
       "      <td>0.005006</td>\n",
       "      <td>12.407407</td>\n",
       "      <td>0.425926</td>\n",
       "      <td>0.62069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>0.112330</td>\n",
       "      <td>0.297348</td>\n",
       "      <td>0.112651</td>\n",
       "      <td>0.012690</td>\n",
       "      <td>3.257576</td>\n",
       "      <td>0.070076</td>\n",
       "      <td>0.283156</td>\n",
       "      <td>0.713881</td>\n",
       "      <td>0.255972</td>\n",
       "      <td>0.065522</td>\n",
       "      <td>8.211520</td>\n",
       "      <td>0.344665</td>\n",
       "      <td>0.176724</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.258333</td>\n",
       "      <td>0.075178</td>\n",
       "      <td>0.005652</td>\n",
       "      <td>5.125000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.520126</td>\n",
       "      <td>0.350611</td>\n",
       "      <td>0.737366</td>\n",
       "      <td>0.134986</td>\n",
       "      <td>0.018221</td>\n",
       "      <td>15.083649</td>\n",
       "      <td>0.551239</td>\n",
       "      <td>0.315453</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.037524</td>\n",
       "      <td>0.001408</td>\n",
       "      <td>9.148148</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.62069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1095 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      DHI_mean   DHI_max   DHI_std   DHI_var   DHI_sum  DHI_median  DNI_mean  \\\n",
       "0     0.140870  0.403409  0.152041  0.023117  4.085227    0.109848  0.123018   \n",
       "1     0.058320  0.126894  0.052898  0.002798  1.691288    0.053030  0.461366   \n",
       "2     0.090713  0.333333  0.101894  0.010382  2.630682    0.053030  0.038520   \n",
       "3     0.123237  0.399621  0.134937  0.018208  3.573864    0.064394  0.104686   \n",
       "4     0.138323  0.412879  0.147664  0.021805  4.011364    0.083333  0.035167   \n",
       "...        ...       ...       ...       ...       ...         ...       ...   \n",
       "1090  0.058712  0.130682  0.053209  0.002831  1.702652    0.064394  0.478949   \n",
       "1091  0.093521  0.257576  0.089891  0.008080  2.712121    0.083333  0.322881   \n",
       "1092  0.108346  0.350379  0.108488  0.011770  3.142045    0.102273  0.267722   \n",
       "1093  0.052377  0.164773  0.047962  0.002300  1.518939    0.064394  0.484224   \n",
       "1094  0.112330  0.297348  0.112651  0.012690  3.257576    0.070076  0.283156   \n",
       "\n",
       "       DNI_max   DNI_std   DNI_var    DNI_sum  DNI_median   WS_mean    WS_min  \\\n",
       "0     0.765817  0.210889  0.044474   3.567517    0.006610  0.175000  0.108333   \n",
       "1     0.925401  0.414326  0.171666  13.379603    0.605288  0.133908  0.033333   \n",
       "2     0.505194  0.116973  0.013683   1.117092    0.000000  0.256609  0.125000   \n",
       "3     0.860246  0.189817  0.036031   3.035883    0.000000  0.208333  0.125000   \n",
       "4     0.207743  0.062211  0.003870   1.019830    0.000000  0.450862  0.191667   \n",
       "...        ...       ...       ...        ...         ...       ...       ...   \n",
       "1090  0.923513  0.406235  0.165027  13.889518    0.654391  0.270115  0.141667   \n",
       "1091  0.744098  0.287719  0.082782   9.363551    0.435316  0.122414  0.041667   \n",
       "1092  0.812087  0.263637  0.069505   7.763928    0.307838  0.276724  0.075000   \n",
       "1093  0.933900  0.405847  0.164711  14.042493    0.693107  0.302299  0.216667   \n",
       "1094  0.713881  0.255972  0.065522   8.211520    0.344665  0.176724  0.050000   \n",
       "\n",
       "        WS_max    WS_std    WS_var     WS_sum  WS_median   RH_mean    RH_min  \\\n",
       "0     0.266667  0.044208  0.001954   5.075000   0.166667  0.629918  0.493020   \n",
       "1     0.275000  0.065197  0.004251   3.883333   0.133333  0.663512  0.499838   \n",
       "2     0.441667  0.096886  0.009387   7.441667   0.216667  0.745624  0.605454   \n",
       "3     0.300000  0.049950  0.002495   6.041667   0.200000  0.583916  0.448977   \n",
       "4     0.666667  0.152449  0.023241  13.075000   0.450000  0.695883  0.615951   \n",
       "...        ...       ...       ...        ...        ...       ...       ...   \n",
       "1090  0.391667  0.070271  0.004938   7.833333   0.258333  0.502767  0.372687   \n",
       "1091  0.250000  0.042789  0.001831   3.550000   0.116667  0.512301  0.357862   \n",
       "1092  0.458333  0.139965  0.019590   8.025000   0.333333  0.428275  0.353533   \n",
       "1093  0.408333  0.080975  0.006557   8.766667   0.291667  0.373030  0.229737   \n",
       "1094  0.258333  0.075178  0.005652   5.125000   0.200000  0.520126  0.350611   \n",
       "\n",
       "        RH_max    RH_std    RH_var     RH_sum  RH_median    T_mean     T_min  \\\n",
       "0     0.748079  0.077597  0.006021  18.267612   0.626123  0.231801  0.129630   \n",
       "1     0.863002  0.111521  0.012437  19.241857   0.663565  0.275223  0.129630   \n",
       "2     0.860946  0.074585  0.005563  21.623093   0.740180  0.272031  0.148148   \n",
       "3     0.703820  0.068998  0.004761  16.933557   0.614327  0.284802  0.129630   \n",
       "4     0.807813  0.058321  0.003401  20.180608   0.695163  0.398467  0.277778   \n",
       "...        ...       ...       ...        ...        ...       ...       ...   \n",
       "1090  0.595282  0.073293  0.005372  14.580240   0.536847  0.371648  0.259259   \n",
       "1091  0.717022  0.115103  0.013249  14.856725   0.484471  0.385696  0.333333   \n",
       "1092  0.546911  0.054368  0.002956  12.419976   0.415215  0.425926  0.351852   \n",
       "1093  0.506439  0.072695  0.005285  10.817877   0.388161  0.427842  0.333333   \n",
       "1094  0.737366  0.134986  0.018221  15.083649   0.551239  0.315453  0.259259   \n",
       "\n",
       "         T_max     T_std     T_var      T_sum  T_median  Day_length  \n",
       "0     0.296296  0.057211  0.003273   6.722222  0.240741     0.62069  \n",
       "1     0.388889  0.088912  0.007905   7.981481  0.296296     0.62069  \n",
       "2     0.351852  0.066788  0.004461   7.888889  0.277778     0.62069  \n",
       "3     0.370370  0.079252  0.006281   8.259259  0.314815     0.62069  \n",
       "4     0.462963  0.055251  0.003053  11.555556  0.407407     0.62069  \n",
       "...        ...       ...       ...        ...       ...         ...  \n",
       "1090  0.462963  0.063175  0.003991  10.777778  0.351852     0.62069  \n",
       "1091  0.481481  0.052860  0.002794  11.185185  0.370370     0.62069  \n",
       "1092  0.518519  0.063382  0.004017  12.351852  0.407407     0.62069  \n",
       "1093  0.537037  0.070750  0.005006  12.407407  0.425926     0.62069  \n",
       "1094  0.370370  0.037524  0.001408   9.148148  0.314815     0.62069  \n",
       "\n",
       "[1095 rows x 34 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Praparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, 203, 6)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 96, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (LSTM)             [(None, 128), (None, 69120       encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_1 (LSTM)           [(None, 96, 128), (N 66560       input_4[0][0]                    \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_2 (LSTM)           [(None, 96, 128), (N 131584      decoder_lstm_1[0][0]             \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_3 (LSTM)           [(None, 96, 128), (N 131584      decoder_lstm_2[0][0]             \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 96, 96)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 96, 238)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 96, 462)      0           decoder_lstm_3[0][0]             \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 96, 256)      118528      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 96, 128)      32896       time_distributed[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 96, 1)        129         time_distributed_1[1][0]         \n",
      "==================================================================================================\n",
      "Total params: 550,401\n",
      "Trainable params: 550,401\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, concatenate, Dropout, BatchNormalization, PReLU\n",
    "from keras.layers import Flatten, Concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras import optimizers\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "# 7일의 데이터로 2일을 예측\n",
    "# 1일 = 48개의 관측값\n",
    "# 7일 = 7 * 48 = 336\n",
    "# 2일 = 2 * 48 = 96\n",
    "def pinball_loss(tau):\n",
    "    def loss(y_true, y_pred):\n",
    "        err = y_true - y_pred\n",
    "        return K.mean(K.maximum(tau * err, (tau - 1) * err), axis=-1)\n",
    "    return loss\n",
    "\n",
    "\n",
    "opt = optimizers.RMSprop()\n",
    "\n",
    "latent_dim = 128\n",
    "\n",
    "encoder_inputs = Input(shape=(29*7, 6), name='encoder_inputs')\n",
    "encoder = LSTM(latent_dim, return_state=True, name='encoder_lstm')\n",
    "encoder_outputs, encoder_state_h, encoder_state_c = encoder(encoder_inputs)\n",
    "# `encoder_outputs`는 버리고 상태(`state_h, state_c`)는 유지\n",
    "encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "# `encoder_states`를 초기 상태로 사용해 decoder를 설정\n",
    "decoder_inputs = Input(shape=(96, 1))\n",
    "decoder_inputs_sequence = Input(shape=(96, 96))\n",
    "decoder_inputs_agg = Input(shape=(96, 34*7))\n",
    "# 전체 출력 시퀀스를 반환하고 내부 상태도 반환하도록 decoder를 설정. \n",
    "# 학습 모델에서 상태를 반환하도록 하진 않지만, inference에서 사용할 예정.\n",
    "decoder_lstm_1 = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm_1')\n",
    "decoder_outputs, _, _ = decoder_lstm_1(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_lstm_2 = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm_2')\n",
    "decoder_outputs, _, _ = decoder_lstm_2(decoder_outputs, initial_state=encoder_states)\n",
    "decoder_lstm_3 = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm_3')\n",
    "decoder_outputs, _, _ = decoder_lstm_3(decoder_outputs, initial_state=encoder_states)\n",
    "\n",
    "concat_inputs = Concatenate()\n",
    "concat_time_inputs = concat_inputs([decoder_outputs, decoder_inputs_sequence, decoder_inputs_agg])\n",
    "\n",
    "decoder_dense_1 = Dense(256)\n",
    "decoder_dense_2 = Dense(128)\n",
    "decoder_dense_3 = Dense(96)\n",
    "\n",
    "decoder_outputs = decoder_time_dense_1(concat_time_inputs)\n",
    "decoder_outputs = decoder_time_dense_2(decoder_outputs)\n",
    "decoder_outputs = decoder_time_dense_3(decoder_outputs)\n",
    "\n",
    "\n",
    "# `encoder_input_data`와 `decoder_input_data`를 `decoder_target_data`로 반환하도록 모델을 정의\n",
    "main_model = Model([encoder_inputs, decoder_inputs, decoder_inputs_sequence, decoder_inputs_agg], decoder_outputs)\n",
    "main_model.compile(loss='MSE', optimizer=opt)\n",
    "main_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1086, 3) (1086, 96, 1)\n",
      "(203, 6) (96, 1) (96, 238)\n"
     ]
    }
   ],
   "source": [
    "def make_train_data_set(df_none_zero, df_agg, ins_day=7, outs_day=2):\n",
    "    df_none_zero = df_none_zero.astype('float')\n",
    "    df_datas = df_none_zero.drop(columns=['Day', 'Origin_TARGET', 'Time']).values\n",
    "    df_agg_datas = df_agg.values\n",
    "    outs_datas = df[['TARGET']].values\n",
    "    \n",
    "    ins = []\n",
    "    outs = []\n",
    "    \n",
    "    term_per_day = 29\n",
    "    \n",
    "    for i in range(len(df_agg)-(ins_day + outs_day)):\n",
    "        encoder_ins_df = df_datas[i*term_per_day:(i+ins_day)*term_per_day]\n",
    "        decoder_outs_df = outs_datas[(i+ins_day)*48:(i+ins_day+outs_day)*48]\n",
    "        decoder_ins_df = np.concatenate(([0], decoder_outs_df[:-1].flatten())).reshape((96, 1))\n",
    "        df_agg_data = df_agg_datas[i:i+7]\n",
    "        ins_df_agg = np.repeat(df_agg_data.reshape(1,34*7), 96, axis=0)\n",
    "        \n",
    "        ins.append([encoder_ins_df, decoder_ins_df, ins_df_agg])\n",
    "        outs.append(decoder_outs_df)\n",
    "        \n",
    "    return np.array(ins), np.array(outs)\n",
    "\n",
    "ins, outs = make_train_data_set(df_none_zero, df_agg, ins_day=7)\n",
    "print(ins.shape, outs.shape)\n",
    "print(ins[0][0].shape, ins[0][1].shape, ins[0][2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(ins, outs, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_encoder = np.array([l for l in x_train[:,0]])\n",
    "x_train_decoder = np.array([l.flatten() for l in x_train[:,1]])\n",
    "x_train_agg = np.array([l for l in x_train[:,2]])\n",
    "\n",
    "x_test_encoder = np.array([l for l in x_test[:,0]])\n",
    "x_test_decoder = np.array([l.flatten() for l in x_test[:,1]])\n",
    "x_test_agg = np.array([l for l in x_test[:,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 96)\n",
      "(868, 96, 96)\n",
      "(218, 96, 96)\n"
     ]
    }
   ],
   "source": [
    "sequence_vector = np.array([[0.0] * 96 for _ in range(96)])\n",
    "for i in range(96):\n",
    "    sequence_vector[i, i] = 1.0\n",
    "\n",
    "print(sequence_vector.shape)\n",
    "train_sequence_vector = np.repeat([sequence_vector], 868, axis=0)\n",
    "print(train_sequence_vector.shape)\n",
    "\n",
    "test_sequence_vector = np.repeat([sequence_vector], 218, axis=0)\n",
    "print(test_sequence_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(868, 203, 6) (868, 96) (868, 96, 96) (868, 96, 238)\n",
      "(218, 203, 6) (218, 96) (218, 96, 96) (218, 96, 238)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_encoder.shape, x_train_decoder.shape, train_sequence_vector.shape, x_train_agg.shape)\n",
    "print(x_test_encoder.shape, x_test_decoder.shape, test_sequence_vector.shape, x_test_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Model ####\n",
      "Epoch 1/500\n",
      "3/3 [==============================] - 1s 349ms/step - loss: 698.9387 - val_loss: 535.6698\n",
      "Epoch 2/500\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 421.7028 - val_loss: 436.8985\n",
      "Epoch 3/500\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 403.7596 - val_loss: 323.1550\n",
      "Epoch 4/500\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 253.2201 - val_loss: 169.9273\n",
      "Epoch 5/500\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 171.6581 - val_loss: 255.8131\n",
      "Epoch 6/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1101\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1103\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \"\"\"\n\u001b[0;32m    439\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    287\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unrecognized hook: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    307\u001b[0m       \u001b[0mbatch_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    340\u001b[0m       \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_supports_tf_logs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Only convert once.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    962\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1014\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m       \u001b[1;31m# Only block async when verbose = 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1016\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1017\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 635\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    531\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m     \"\"\"\n\u001b[0;32m   1062\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1027\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "print(f'#### Model ####')\n",
    "his = main_model.fit([x_train_encoder, x_train_decoder, train_sequence_vector, x_train_agg], y_train, epochs=500, batch_size=300, \n",
    "                     validation_data=([x_test_encoder, x_test_decoder, test_sequence_vector, x_test_agg], y_test), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(his.history['loss'][50:])\n",
    "plt.plot(his.history['val_loss'][50:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 1, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pre_decoder_lstm_1 (LSTM)       [(None, 1, 128), (No 66560       input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pre_decoder_lstm_2 (LSTM)       [(None, 1, 128), (No 131584      pre_decoder_lstm_1[0][0]         \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pre_decoder_lstm_3 (LSTM)       [(None, 128), (None, 131584      pre_decoder_lstm_2[0][0]         \n",
      "                                                                 input_10[0][0]                   \n",
      "                                                                 input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pre_decoder_inputs_sequence (In [(None, 96)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 224)          0           pre_decoder_lstm_3[0][0]         \n",
      "                                                                 pre_decoder_inputs_sequence[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            225         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 329,953\n",
      "Trainable params: 329,953\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "pre_decoder_inputs = Input(shape=(1, 1))\n",
    "pre_decoder_state_input_h_1 = Input(shape=(latent_dim,))\n",
    "pre_decoder_state_input_c_1 = Input(shape=(latent_dim,))\n",
    "pre_decoder_state_input_h_2 = Input(shape=(latent_dim,))\n",
    "pre_decoder_state_input_c_2 = Input(shape=(latent_dim,))\n",
    "pre_decoder_state_input_h_3 = Input(shape=(latent_dim,))\n",
    "pre_decoder_state_input_c_3 = Input(shape=(latent_dim,))\n",
    "\n",
    "pre_decoder_states_inputs = [\n",
    "    pre_decoder_state_input_h_1, pre_decoder_state_input_c_1,\n",
    "    pre_decoder_state_input_h_2, pre_decoder_state_input_c_2,\n",
    "    pre_decoder_state_input_h_3, pre_decoder_state_input_c_3,\n",
    "]\n",
    "\n",
    "pre_decoder_lstm_1 = LSTM(latent_dim, return_sequences=True, return_state=True, name='pre_decoder_lstm_1')\n",
    "pre_decoder_lstm_2 = LSTM(latent_dim, return_sequences=True, return_state=True, name='pre_decoder_lstm_2')\n",
    "pre_decoder_lstm_3 = LSTM(latent_dim, return_sequences=False, return_state=True, name='pre_decoder_lstm_3')\n",
    "\n",
    "pre_decoder_outputs, pre_decoder_state_h_1, pre_decoder_state_c_1 = pre_decoder_lstm_1(pre_decoder_inputs, initial_state=pre_decoder_states_inputs[:2])\n",
    "pre_decoder_outputs, pre_decoder_state_h_2, pre_decoder_state_c_2 = pre_decoder_lstm_2(pre_decoder_outputs, initial_state=pre_decoder_states_inputs[2:4])\n",
    "pre_decoder_outputs, pre_decoder_state_h_3, pre_decoder_state_c_3 = pre_decoder_lstm_3(pre_decoder_outputs, initial_state=pre_decoder_states_inputs[4:])\n",
    "\n",
    "pre_decoder_states = [\n",
    "    pre_decoder_state_h_1, pre_decoder_state_c_1,\n",
    "    pre_decoder_state_h_2, pre_decoder_state_c_2,\n",
    "    pre_decoder_state_h_3, pre_decoder_state_c_3,\n",
    "]\n",
    "\n",
    "pre_decoder_inputs_sequence = Input(shape=(96, ), name='pre_decoder_inputs_sequence')\n",
    "\n",
    "\n",
    "pre_concat_inputs = Concatenate()\n",
    "pre_concat_time_inputs = pre_concat_inputs([pre_decoder_outputs, pre_decoder_inputs_sequence])\n",
    "\n",
    "pre_decoder_outputs = decoder_dense(pre_concat_time_inputs)\n",
    "# decoder_outputs = fine_decoder_dense_2(decoder_outputs)\n",
    "# decoder_outputs = fine_decoder_dense_3(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [pre_decoder_inputs, pre_decoder_inputs_sequence] + pre_decoder_states_inputs,\n",
    "    [pre_decoder_outputs] + pre_decoder_states\n",
    ")\n",
    "\n",
    "decoder_model.summary()\n",
    "\n",
    "decoder_model.get_layer('pre_decoder_lstm_1').set_weights(main_model.get_layer('decoder_lstm_1').get_weights())\n",
    "decoder_model.get_layer('pre_decoder_lstm_2').set_weights(main_model.get_layer('decoder_lstm_2').get_weights())\n",
    "decoder_model.get_layer('pre_decoder_lstm_3').set_weights(main_model.get_layer('decoder_lstm_3').get_weights())\n",
    "# decoder_model.get_layer('last_dense_1').set_weights(fine_model.get_layer('time_dense_1').get_weights())\n",
    "# decoder_model.get_layer('last_dense_2').set_weights(fine_model.get_layer('time_dense_2').get_weights())\n",
    "# decoder_model.get_layer('last_dense_3').set_weights(fine_model.get_layer('time_dense_3').get_weights())\n",
    "# decoder_model.get_layer('pre_decoder_lstm_2').set_weights(main_model.get_layer('decoder_lstm_2').get_weights())\n",
    "# decoder_model.get_layer('pre_decoder_lstm_3').set_weights(main_model.get_layer('decoder_lstm_3').get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 상태 벡터로서 입력값을 encode\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    result = []\n",
    "\n",
    "    target_seq = np.zeros((1, 1, 1))\n",
    "    target_seq[0, 0, 0] = 0.0\n",
    "    \n",
    "    for i in range(96):\n",
    "        output, h1, c1, h2, c2, h3, c3 = decoder_model.predict([target_seq, sequence_vector[i].reshape((1, 96))] + states_value*3)\n",
    "        \n",
    "        \n",
    "        # 토큰으로 샘플링\n",
    "        target_out = output[0, 0]\n",
    "#         print(output)\n",
    "        result.append(target_out)\n",
    "\n",
    "        # (길이 1인) 목표 시퀀스 최신화\n",
    "        target_seq = np.zeros((1, 1, 1))\n",
    "        target_seq[0, 0, 0] = target_out\n",
    "\n",
    "        # 상태 최신화\n",
    "        states_value = [h1, c1, h2, c2, h3, c3]\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, 203, 6)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           [(None, 96, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (LSTM)             [(None, 128), (None, 69120       encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_1 (LSTM)           [(None, 96, 128), (N 66560       input_14[0][0]                   \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_2 (LSTM)           [(None, 96, 128), (N 131584      decoder_lstm_1[2][0]             \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_3 (LSTM)           [(None, 96, 128), (N 131584      decoder_lstm_2[2][0]             \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           [(None, 96, 96)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 96, 224)      0           decoder_lstm_3[2][0]             \n",
      "                                                                 input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_dense_1 (TimeDistributed)  (None, 96, 150)      33750       concatenate[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_dense_2 (TimeDistributed)  (None, 96, 100)      15100       time_dense_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_dense_3 (TimeDistributed)  (None, 96, 1)        101         time_dense_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 447,799\n",
      "Trainable params: 48,951\n",
      "Non-trainable params: 398,848\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = optimizers.Adam()\n",
    "\n",
    "fine_decoder_inputs = Input(shape=(96, 1))\n",
    "fine_decoder_inputs_sequence = Input(shape=(96, 96))\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm_1(fine_decoder_inputs, initial_state=encoder_states)\n",
    "decoder_outputs, _, _ = decoder_lstm_2(decoder_outputs, initial_state=encoder_states)\n",
    "decoder_outputs, _, _ = decoder_lstm_3(decoder_outputs, initial_state=encoder_states)\n",
    "\n",
    "concat_time_inputs = concat_inputs([decoder_outputs, fine_decoder_inputs_sequence])\n",
    "\n",
    "fine_decoder_dense_1 = Dense(150, name='dense_1')\n",
    "decoder_time_dense_1 = TimeDistributed(fine_decoder_dense_1, name='time_dense_1')\n",
    "\n",
    "fine_decoder_dense_2 = Dense(100, name='dense_2')\n",
    "decoder_time_dense_2 = TimeDistributed(fine_decoder_dense_2, name='time_dense_2')\n",
    "\n",
    "fine_decoder_dense_3 = Dense(1, name='dense_3', activation='linear')\n",
    "decoder_time_dense_3 = TimeDistributed(fine_decoder_dense_3, name='time_dense_3')\n",
    "\n",
    "\n",
    "decoder_outputs = decoder_time_dense_1(concat_time_inputs)\n",
    "decoder_outputs = decoder_time_dense_2(decoder_outputs)\n",
    "decoder_outputs = decoder_time_dense_3(decoder_outputs)\n",
    "\n",
    "# `encoder_input_data`와 `decoder_input_data`를 `decoder_target_data`로 반환하도록 모델을 정의\n",
    "fine_model = Model([encoder_inputs, fine_decoder_inputs, fine_decoder_inputs_sequence], decoder_outputs)\n",
    "fine_model.compile(loss='MSE', optimizer=opt)\n",
    "\n",
    "#     model.compile(loss=pinball_loss(tau), optimizer=opt)\n",
    "# if use_mse:\n",
    "\n",
    "# fine_main_model.get_layer('fine_decoder_lstm_1').set_weights(main_model.get_layer('decoder_lstm_1').get_weights())\n",
    "fine_model.get_layer('decoder_lstm_1').trainable=False\n",
    "fine_model.get_layer('decoder_lstm_2').trainable=False\n",
    "fine_model.get_layer('decoder_lstm_3').trainable=False\n",
    "fine_model.get_layer('encoder_lstm').trainable=False\n",
    "\n",
    "fine_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Model ####\n",
      "Epoch 1/100\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 852.0349 - val_loss: 701.9185\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 613.8353 - val_loss: 465.3910\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 379.3287 - val_loss: 252.5028\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 184.4000 - val_loss: 115.1856\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 83.5545 - val_loss: 108.4548\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 102.4783 - val_loss: 163.6601\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 135.4930 - val_loss: 147.5782\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 103.0780 - val_loss: 96.2706\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 61.8093 - val_loss: 73.2529\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 51.5290 - val_loss: 77.1793\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 59.1288 - val_loss: 82.6071\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 61.3656 - val_loss: 77.9756\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 53.8701 - val_loss: 69.5392\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 45.4845 - val_loss: 66.7852\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 43.5294 - val_loss: 70.2601\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 45.7519 - val_loss: 72.2041\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 45.9467 - val_loss: 69.6801\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 43.4990 - val_loss: 66.7753\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 42.0987 - val_loss: 66.1282\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 42.5126 - val_loss: 66.3878\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 42.8334 - val_loss: 66.1160\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 42.2448 - val_loss: 65.7292\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 41.5985 - val_loss: 65.9576\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 41.5786 - val_loss: 66.3567\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 41.6379 - val_loss: 66.1746\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 41.4246 - val_loss: 65.6476\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 41.1834 - val_loss: 65.2643\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 41.1392 - val_loss: 65.1098\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 41.1143 - val_loss: 65.0472\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 41.0005 - val_loss: 65.0619\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.8907 - val_loss: 65.1515\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.8461 - val_loss: 65.1999\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.8108 - val_loss: 65.1231\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.7498 - val_loss: 64.9988\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.6922 - val_loss: 64.8913\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.6532 - val_loss: 64.8374\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 40.6156 - val_loss: 64.8304\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 40.5700 - val_loss: 64.8504\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 40.5338 - val_loss: 64.8796\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 40.5063 - val_loss: 64.8751\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 40.4734 - val_loss: 64.8163\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 40.4402 - val_loss: 64.7541\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 40.4137 - val_loss: 64.7162\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.3886 - val_loss: 64.7120\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.3645 - val_loss: 64.7309\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.3376 - val_loss: 64.7201\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.3158 - val_loss: 64.7048\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.2938 - val_loss: 64.6845\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.2747 - val_loss: 64.6674\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.2555 - val_loss: 64.6601\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.2373 - val_loss: 64.6456\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.2194 - val_loss: 64.6415\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.2039 - val_loss: 64.6360\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 40.1873 - val_loss: 64.6276\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 40.1725 - val_loss: 64.6225\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 40.1599 - val_loss: 64.6295\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 40.1493 - val_loss: 64.5884\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 40.1316 - val_loss: 64.5945\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 40.1186 - val_loss: 64.6005\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 40.1066 - val_loss: 64.6049\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.0968 - val_loss: 64.6143\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.0840 - val_loss: 64.5902\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.0731 - val_loss: 64.5776\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.0634 - val_loss: 64.5698\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.0532 - val_loss: 64.5706\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.0442 - val_loss: 64.5759\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.0342 - val_loss: 64.5699\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 40.0261 - val_loss: 64.5799\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 40.0175 - val_loss: 64.5723\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 40.0088 - val_loss: 64.5787\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 40.0013 - val_loss: 64.5770\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 39.9943 - val_loss: 64.5587\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 39.9868 - val_loss: 64.5759\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 39.9793 - val_loss: 64.5765\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 39.9720 - val_loss: 64.5641\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 39.9654 - val_loss: 64.5657\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 39.9591 - val_loss: 64.5660\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 39.9530 - val_loss: 64.5628\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 39.9470 - val_loss: 64.5759\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 39.9404 - val_loss: 64.5698\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 39.9352 - val_loss: 64.5731\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 39.9296 - val_loss: 64.5769\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 39.9241 - val_loss: 64.5637\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 39.9223 - val_loss: 64.5461\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 39.9138 - val_loss: 64.5809\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 39.9109 - val_loss: 64.6236\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 39.9056 - val_loss: 64.6014\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 39.8990 - val_loss: 64.5692\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 39.8949 - val_loss: 64.5582\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 39.8910 - val_loss: 64.5736\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 39.8860 - val_loss: 64.5882\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 39.8826 - val_loss: 64.6173\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 39.8783 - val_loss: 64.5959\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 39.8741 - val_loss: 64.5768\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 39.8707 - val_loss: 64.5826\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 39.8663 - val_loss: 64.6067\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 39.8623 - val_loss: 64.6085\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 39.8586 - val_loss: 64.6102\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 39.8550 - val_loss: 64.6037\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 39.8522 - val_loss: 64.5984\n"
     ]
    }
   ],
   "source": [
    "print(f'#### Model ####')\n",
    "his = fine_model.fit([x_train_encoder, x_train_decoder, train_sequence_vector], y_train, epochs=100, batch_size=300, \n",
    "#                       verbose=1)\n",
    "                     validation_data=([x_test_encoder, x_test_decoder, test_sequence_vector], y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_15\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_23 (InputLayer)           [(None, 1, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_24 (InputLayer)           [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_25 (InputLayer)           [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pre_decoder_lstm_1 (LSTM)       [(None, 1, 128), (No 66560       input_23[0][0]                   \n",
      "                                                                 input_24[0][0]                   \n",
      "                                                                 input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_27 (InputLayer)           [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pre_decoder_lstm_2 (LSTM)       [(None, 1, 128), (No 131584      pre_decoder_lstm_1[0][0]         \n",
      "                                                                 input_26[0][0]                   \n",
      "                                                                 input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_29 (InputLayer)           [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pre_decoder_lstm_3 (LSTM)       [(None, 128), (None, 131584      pre_decoder_lstm_2[0][0]         \n",
      "                                                                 input_28[0][0]                   \n",
      "                                                                 input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pre_decoder_inputs_sequence (In [(None, 96)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 224)          0           pre_decoder_lstm_3[0][0]         \n",
      "                                                                 pre_decoder_inputs_sequence[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 150)          33750       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          15100       dense_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            101         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 378,679\n",
      "Trainable params: 378,679\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "pre_decoder_inputs = Input(shape=(1, 1))\n",
    "pre_decoder_state_input_h_1 = Input(shape=(latent_dim,))\n",
    "pre_decoder_state_input_c_1 = Input(shape=(latent_dim,))\n",
    "pre_decoder_state_input_h_2 = Input(shape=(latent_dim,))\n",
    "pre_decoder_state_input_c_2 = Input(shape=(latent_dim,))\n",
    "pre_decoder_state_input_h_3 = Input(shape=(latent_dim,))\n",
    "pre_decoder_state_input_c_3 = Input(shape=(latent_dim,))\n",
    "\n",
    "pre_decoder_states_inputs = [\n",
    "    pre_decoder_state_input_h_1, pre_decoder_state_input_c_1,\n",
    "    pre_decoder_state_input_h_2, pre_decoder_state_input_c_2,\n",
    "    pre_decoder_state_input_h_3, pre_decoder_state_input_c_3,\n",
    "]\n",
    "\n",
    "pre_decoder_lstm_1 = LSTM(latent_dim, return_sequences=True, return_state=True, name='pre_decoder_lstm_1')\n",
    "pre_decoder_lstm_2 = LSTM(latent_dim, return_sequences=True, return_state=True, name='pre_decoder_lstm_2')\n",
    "pre_decoder_lstm_3 = LSTM(latent_dim, return_sequences=False, return_state=True, name='pre_decoder_lstm_3')\n",
    "\n",
    "pre_decoder_outputs, pre_decoder_state_h_1, pre_decoder_state_c_1 = pre_decoder_lstm_1(pre_decoder_inputs, initial_state=pre_decoder_states_inputs[:2])\n",
    "pre_decoder_outputs, pre_decoder_state_h_2, pre_decoder_state_c_2 = pre_decoder_lstm_2(pre_decoder_outputs, initial_state=pre_decoder_states_inputs[2:4])\n",
    "pre_decoder_outputs, pre_decoder_state_h_3, pre_decoder_state_c_3 = pre_decoder_lstm_3(pre_decoder_outputs, initial_state=pre_decoder_states_inputs[4:])\n",
    "\n",
    "pre_decoder_states = [\n",
    "    pre_decoder_state_h_1, pre_decoder_state_c_1,\n",
    "    pre_decoder_state_h_2, pre_decoder_state_c_2,\n",
    "    pre_decoder_state_h_3, pre_decoder_state_c_3,\n",
    "]\n",
    "\n",
    "pre_decoder_inputs_sequence = Input(shape=(96, ), name='pre_decoder_inputs_sequence')\n",
    "\n",
    "\n",
    "pre_concat_inputs = Concatenate()\n",
    "pre_concat_time_inputs = pre_concat_inputs([pre_decoder_outputs, pre_decoder_inputs_sequence])\n",
    "\n",
    "pre_decoder_outputs = fine_decoder_dense_1(pre_concat_time_inputs)\n",
    "pre_decoder_outputs = fine_decoder_dense_2(pre_decoder_outputs)\n",
    "pre_decoder_outputs = fine_decoder_dense_3(pre_decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [pre_decoder_inputs, pre_decoder_inputs_sequence] + pre_decoder_states_inputs,\n",
    "    [pre_decoder_outputs] + pre_decoder_states\n",
    ")\n",
    "\n",
    "decoder_model.get_layer('pre_decoder_lstm_1').set_weights(main_model.get_layer('decoder_lstm_1').get_weights())\n",
    "decoder_model.get_layer('pre_decoder_lstm_2').set_weights(main_model.get_layer('decoder_lstm_2').get_weights())\n",
    "decoder_model.get_layer('pre_decoder_lstm_3').set_weights(main_model.get_layer('decoder_lstm_3').get_weights())\n",
    "\n",
    "decoder_model.summary()\n",
    "# decoder_model.get_layer('last_dense_1').set_weights(fine_model.get_layer('time_dense_1').get_weights())\n",
    "# decoder_model.get_layer('last_dense_2').set_weights(fine_model.get_layer('time_dense_2').get_weights())\n",
    "# decoder_model.get_layer('last_dense_3').set_weights(fine_model.get_layer('time_dense_3').get_weights())\n",
    "# decoder_model.get_layer('pre_decoder_lstm_2').set_weights(main_model.get_layer('decoder_lstm_2').get_weights())\n",
    "# decoder_model.get_layer('pre_decoder_lstm_3').set_weights(main_model.get_layer('decoder_lstm_3').get_weights())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
